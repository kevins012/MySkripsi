{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32e5171",
   "metadata": {},
   "source": [
    "# EDA_S \n",
    "File Ipynb yang berisi EDA dan pelatihan Model pada Data Statis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d46608",
   "metadata": {},
   "source": [
    "# Daftar Isi\n",
    "\n",
    "1. [Import Library and Dependency](#IMPORTLIBRARY)\n",
    "2. [Extract Raw Landmark](#EXTRACT-RAW-LANDMARK)\n",
    "3. [Combined Dataset](#COMBINED-DATASET-I)\n",
    "4. [Fungsi Normalisasi](#FUNGSI-NORMALISASI)\n",
    "5. [File And Folder](#FILE-AND-FOLDER-EXPLAINED)\n",
    "6. [Drop Kolom Tertentu](#DROP-KOLOM-TERTENTU)\n",
    "7. [Preprocessing Data pada CSV](#PREPROCESSING-DATA-PADA-CSV)\n",
    "8. [Extract Data with Normalisation](#EXTRACT-DATA-WITH-NORMALISATION)\n",
    "9. [Import Library to Create Model Architecture](#IMPORT-LIBRARY-TO-CREATE-MODEL-ARCHITECTURE)\n",
    "10. [Split Data](#SPLIT-DATA)\n",
    "11. [Create Label Map untuk Implementasi Nanti dan Save to PKL and Load](#CREATE-LABEL-MAP-UNTUK-IMPLEMENTASI-NANTI-DAN-SAVE-TO-PKL-AND-LOAD)\n",
    "12. [Encoded and to Categori Label](#ENCODED-AND-TO-CATEGORI-LABEL)\n",
    "13. [Transform DataFrame Pandas CSV to Numpy Array](#TRANSFORM-DATAFRAME-PANDAS-CSV-TO-NUMPY-ARRAY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961960d6",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fda8f405-acfb-4193-a047-1460046bde64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 22:02:50.407601: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-10 22:02:50.558775: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746889370.641691    4382 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746889370.668377    4382 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746889370.796565    4382 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746889370.796615    4382 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746889370.796622    4382 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746889370.796627    4382 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-10 22:02:50.817903: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf0\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec780a8-aad6-4ddc-abc2-a4c8a1bfeb1a",
   "metadata": {},
   "source": [
    "## EXTRACT RAW LANDMARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "73620da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746896506.225261    4382 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1746896506.226751  394341 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) Graphics (RPL-S)\n",
      "W0000 00:00:1746896506.245453  394344 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746896506.259589  394350 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next\n",
      "selesai\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import mediapipe as mpfil\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "\n",
    "# Direktori gambar\n",
    "\n",
    "\n",
    "# Inisialisasi MediaPipe HandLandmarker\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    num_hands=2,\n",
    "    running_mode=vision.RunningMode.IMAGE\n",
    ")\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "max_images = 38\n",
    "path = 'img'\n",
    "\n",
    "# Data untuk DataFrame\n",
    "folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f)) and not f.startswith('.')]\n",
    "folders=['z']\n",
    "data = []\n",
    "for target in folders:\n",
    "    \n",
    "    data_separated =[]\n",
    "    image_directory = path+\"/\"+target\n",
    "    img_path = sorted([f for f in os.listdir(image_directory) if f.endswith(('.jpg', '.png'))],key=lambda x: int(x.split('.')[0])) \n",
    "    a=1\n",
    "    # img_path = [sorted([f for f in os.listdir(image_directory) if f.endswith(('.jpg', '.png'))],key=lambda x: int(x.split('.')[0]))[i] for i in [2, 40, 70,112]]\n",
    "    for img in img_path:\n",
    "        image_path = os.path.join(image_directory, img)\n",
    "        image = cv2.imread(image_path )\n",
    "        \n",
    "        if image is None:\n",
    "            print(f\"Gagal membuka {image_path } i\")\n",
    "            continue\n",
    "    \n",
    "        # Konversi ke RGB untuk MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "    \n",
    "        # Deteksi tangan\n",
    "        detection_result = detector.detect(mp_image)\n",
    "        if not detection_result.hand_landmarks:\n",
    "            print(f\"Tidak ada tangan terdeteksi dalam {image_path } in label {target} \")\n",
    "            os.remove(image_path)\n",
    "         \n",
    "        \n",
    "        # Format data sesuai permintaan\n",
    "        for hand_idx, hand in enumerate(detection_result.hand_landmarks):\n",
    "     \n",
    "            x_coords  = np.array([landmark.x for landmark in hand])\n",
    "            y_coords = np.array([landmark.y for landmark in hand])\n",
    "            z_coords = np.array([landmark.z for landmark in hand])\n",
    "\n",
    "            row_data = {f\"X{idx}\": x for idx, x in zip(range(21),x_coords)}\n",
    "            row_data.update({f\"Y{idx}\": y for idx, y in zip(range(21),y_coords)})\n",
    "            row_data.update({f\"Z{idx}\": z for idx,z in zip(range(21),z_coords)})\n",
    "            \n",
    "            \n",
    "            row_data[\"Sample Num\"] = a\n",
    "            row_data[\"Label\"] = target\n",
    "            \n",
    "\n",
    "            data.append(row_data)\n",
    "            data_separated.append(row_data)\n",
    "        a+=1\n",
    "    pd.DataFrame(data_separated).to_csv(f'csv_separated/static/{target}/pure.csv',index=False)\n",
    "    print('next')\n",
    "df = pd.DataFrame(data)\n",
    "print('selesai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "757decc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'csv/static/pure.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab32ea8a",
   "metadata": {},
   "source": [
    "## COMBINED DATASET I\n",
    "* Menggabungkan Dataset pada file csv di semua folder yang ada didalam direktori **csv_separated/static** *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e70828d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penggabungan selesai! File disimpan dengan nama 'combined_pure.csv'\n"
     ]
    }
   ],
   "source": [
    "# Tentukan path ke folder utama\n",
    "folder_path = 'csv_separated/static/'\n",
    "# folder_path = 'csv_separated/angka/'\n",
    "# Daftar untuk menyimpan DataFrame yang dibaca dari setiap file CSV\n",
    "dfs = []\n",
    "\n",
    "# Loop untuk mencari semua file 'pure.csv' dalam subfolder\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    if 'pure.csv' in files:  # Cek jika ada file 'pure.csv' di folder tersebut\n",
    "        file_path = os.path.join(root, 'pure.csv')  # Dapatkan path lengkap file\n",
    "        df = pd.read_csv(file_path)  # Baca file CSV\n",
    "        dfs.append(df)  # Tambahkan DataFrame ke dalam list\n",
    "\n",
    "# Gabungkan semua DataFrame secara vertikal\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Simpan hasil gabungan ke dalam file baru (misalnya 'combined_pure.csv')\n",
    "combined_df.to_csv('csv/static/pure.csv', index=False)\n",
    "\n",
    "print(\"Penggabungan selesai! File disimpan dengan nama 'combined_pure.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cbc5466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>Z13</th>\n",
       "      <th>Z14</th>\n",
       "      <th>Z15</th>\n",
       "      <th>Z16</th>\n",
       "      <th>Z17</th>\n",
       "      <th>Z18</th>\n",
       "      <th>Z19</th>\n",
       "      <th>Z20</th>\n",
       "      <th>Sample Num</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.254282</td>\n",
       "      <td>0.327569</td>\n",
       "      <td>0.377404</td>\n",
       "      <td>0.351012</td>\n",
       "      <td>0.290836</td>\n",
       "      <td>0.344270</td>\n",
       "      <td>0.337857</td>\n",
       "      <td>0.340832</td>\n",
       "      <td>0.341920</td>\n",
       "      <td>0.295675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027655</td>\n",
       "      <td>-0.061845</td>\n",
       "      <td>-0.075178</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.037640</td>\n",
       "      <td>-0.060164</td>\n",
       "      <td>-0.067015</td>\n",
       "      <td>-0.067270</td>\n",
       "      <td>1</td>\n",
       "      <td>titik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278144</td>\n",
       "      <td>0.348619</td>\n",
       "      <td>0.399805</td>\n",
       "      <td>0.380309</td>\n",
       "      <td>0.324560</td>\n",
       "      <td>0.366909</td>\n",
       "      <td>0.362039</td>\n",
       "      <td>0.367706</td>\n",
       "      <td>0.366957</td>\n",
       "      <td>0.318644</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026346</td>\n",
       "      <td>-0.057128</td>\n",
       "      <td>-0.068359</td>\n",
       "      <td>-0.067404</td>\n",
       "      <td>-0.037278</td>\n",
       "      <td>-0.056616</td>\n",
       "      <td>-0.061879</td>\n",
       "      <td>-0.060964</td>\n",
       "      <td>2</td>\n",
       "      <td>titik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.307278</td>\n",
       "      <td>0.373972</td>\n",
       "      <td>0.428036</td>\n",
       "      <td>0.416791</td>\n",
       "      <td>0.363796</td>\n",
       "      <td>0.391103</td>\n",
       "      <td>0.386037</td>\n",
       "      <td>0.395147</td>\n",
       "      <td>0.393565</td>\n",
       "      <td>0.343444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031913</td>\n",
       "      <td>-0.061052</td>\n",
       "      <td>-0.070387</td>\n",
       "      <td>-0.068704</td>\n",
       "      <td>-0.042430</td>\n",
       "      <td>-0.063148</td>\n",
       "      <td>-0.069241</td>\n",
       "      <td>-0.068755</td>\n",
       "      <td>3</td>\n",
       "      <td>titik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.301815</td>\n",
       "      <td>0.368061</td>\n",
       "      <td>0.422138</td>\n",
       "      <td>0.415346</td>\n",
       "      <td>0.361173</td>\n",
       "      <td>0.384891</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.388489</td>\n",
       "      <td>0.389061</td>\n",
       "      <td>0.336701</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033412</td>\n",
       "      <td>-0.064247</td>\n",
       "      <td>-0.072699</td>\n",
       "      <td>-0.070425</td>\n",
       "      <td>-0.044378</td>\n",
       "      <td>-0.065955</td>\n",
       "      <td>-0.071458</td>\n",
       "      <td>-0.070688</td>\n",
       "      <td>4</td>\n",
       "      <td>titik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.282829</td>\n",
       "      <td>0.349927</td>\n",
       "      <td>0.399580</td>\n",
       "      <td>0.382175</td>\n",
       "      <td>0.326279</td>\n",
       "      <td>0.364495</td>\n",
       "      <td>0.358810</td>\n",
       "      <td>0.365505</td>\n",
       "      <td>0.366256</td>\n",
       "      <td>0.316976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028226</td>\n",
       "      <td>-0.058030</td>\n",
       "      <td>-0.067803</td>\n",
       "      <td>-0.065806</td>\n",
       "      <td>-0.039263</td>\n",
       "      <td>-0.058188</td>\n",
       "      <td>-0.062696</td>\n",
       "      <td>-0.061009</td>\n",
       "      <td>5</td>\n",
       "      <td>titik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.277146</td>\n",
       "      <td>0.329509</td>\n",
       "      <td>0.364196</td>\n",
       "      <td>0.343807</td>\n",
       "      <td>0.299967</td>\n",
       "      <td>0.346478</td>\n",
       "      <td>0.342900</td>\n",
       "      <td>0.342538</td>\n",
       "      <td>0.335413</td>\n",
       "      <td>0.309070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021119</td>\n",
       "      <td>-0.045204</td>\n",
       "      <td>-0.054065</td>\n",
       "      <td>-0.054343</td>\n",
       "      <td>-0.026302</td>\n",
       "      <td>-0.041809</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.042228</td>\n",
       "      <td>111</td>\n",
       "      <td>titik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.243805</td>\n",
       "      <td>0.310951</td>\n",
       "      <td>0.361250</td>\n",
       "      <td>0.365126</td>\n",
       "      <td>0.318592</td>\n",
       "      <td>0.339763</td>\n",
       "      <td>0.344734</td>\n",
       "      <td>0.355133</td>\n",
       "      <td>0.354371</td>\n",
       "      <td>0.301135</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022421</td>\n",
       "      <td>-0.039864</td>\n",
       "      <td>-0.050452</td>\n",
       "      <td>-0.054408</td>\n",
       "      <td>-0.033516</td>\n",
       "      <td>-0.047402</td>\n",
       "      <td>-0.053279</td>\n",
       "      <td>-0.055458</td>\n",
       "      <td>112</td>\n",
       "      <td>titik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.241557</td>\n",
       "      <td>0.305792</td>\n",
       "      <td>0.350882</td>\n",
       "      <td>0.345407</td>\n",
       "      <td>0.295993</td>\n",
       "      <td>0.331015</td>\n",
       "      <td>0.330394</td>\n",
       "      <td>0.342029</td>\n",
       "      <td>0.344380</td>\n",
       "      <td>0.290435</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028354</td>\n",
       "      <td>-0.051737</td>\n",
       "      <td>-0.063883</td>\n",
       "      <td>-0.067588</td>\n",
       "      <td>-0.038103</td>\n",
       "      <td>-0.053902</td>\n",
       "      <td>-0.059766</td>\n",
       "      <td>-0.061661</td>\n",
       "      <td>113</td>\n",
       "      <td>titik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.230343</td>\n",
       "      <td>0.298052</td>\n",
       "      <td>0.344912</td>\n",
       "      <td>0.345336</td>\n",
       "      <td>0.299191</td>\n",
       "      <td>0.313065</td>\n",
       "      <td>0.308647</td>\n",
       "      <td>0.322514</td>\n",
       "      <td>0.326814</td>\n",
       "      <td>0.273508</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028631</td>\n",
       "      <td>-0.049788</td>\n",
       "      <td>-0.061275</td>\n",
       "      <td>-0.065302</td>\n",
       "      <td>-0.039899</td>\n",
       "      <td>-0.056716</td>\n",
       "      <td>-0.063922</td>\n",
       "      <td>-0.066518</td>\n",
       "      <td>114</td>\n",
       "      <td>titik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.232678</td>\n",
       "      <td>0.291139</td>\n",
       "      <td>0.326120</td>\n",
       "      <td>0.301383</td>\n",
       "      <td>0.250978</td>\n",
       "      <td>0.303656</td>\n",
       "      <td>0.297742</td>\n",
       "      <td>0.305560</td>\n",
       "      <td>0.306313</td>\n",
       "      <td>0.262955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031244</td>\n",
       "      <td>-0.056168</td>\n",
       "      <td>-0.066515</td>\n",
       "      <td>-0.068234</td>\n",
       "      <td>-0.041803</td>\n",
       "      <td>-0.060606</td>\n",
       "      <td>-0.066032</td>\n",
       "      <td>-0.066580</td>\n",
       "      <td>115</td>\n",
       "      <td>titik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X0        X1        X2        X3        X4        X5        X6  \\\n",
       "0    0.254282  0.327569  0.377404  0.351012  0.290836  0.344270  0.337857   \n",
       "1    0.278144  0.348619  0.399805  0.380309  0.324560  0.366909  0.362039   \n",
       "2    0.307278  0.373972  0.428036  0.416791  0.363796  0.391103  0.386037   \n",
       "3    0.301815  0.368061  0.422138  0.415346  0.361173  0.384891  0.378800   \n",
       "4    0.282829  0.349927  0.399580  0.382175  0.326279  0.364495  0.358810   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "110  0.277146  0.329509  0.364196  0.343807  0.299967  0.346478  0.342900   \n",
       "111  0.243805  0.310951  0.361250  0.365126  0.318592  0.339763  0.344734   \n",
       "112  0.241557  0.305792  0.350882  0.345407  0.295993  0.331015  0.330394   \n",
       "113  0.230343  0.298052  0.344912  0.345336  0.299191  0.313065  0.308647   \n",
       "114  0.232678  0.291139  0.326120  0.301383  0.250978  0.303656  0.297742   \n",
       "\n",
       "           X7        X8        X9  ...       Z13       Z14       Z15  \\\n",
       "0    0.340832  0.341920  0.295675  ... -0.027655 -0.061845 -0.075178   \n",
       "1    0.367706  0.366957  0.318644  ... -0.026346 -0.057128 -0.068359   \n",
       "2    0.395147  0.393565  0.343444  ... -0.031913 -0.061052 -0.070387   \n",
       "3    0.388489  0.389061  0.336701  ... -0.033412 -0.064247 -0.072699   \n",
       "4    0.365505  0.366256  0.316976  ... -0.028226 -0.058030 -0.067803   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "110  0.342538  0.335413  0.309070  ... -0.021119 -0.045204 -0.054065   \n",
       "111  0.355133  0.354371  0.301135  ... -0.022421 -0.039864 -0.050452   \n",
       "112  0.342029  0.344380  0.290435  ... -0.028354 -0.051737 -0.063883   \n",
       "113  0.322514  0.326814  0.273508  ... -0.028631 -0.049788 -0.061275   \n",
       "114  0.305560  0.306313  0.262955  ... -0.031244 -0.056168 -0.066515   \n",
       "\n",
       "          Z16       Z17       Z18       Z19       Z20  Sample Num  Label  \n",
       "0   -0.075484 -0.037640 -0.060164 -0.067015 -0.067270           1  titik  \n",
       "1   -0.067404 -0.037278 -0.056616 -0.061879 -0.060964           2  titik  \n",
       "2   -0.068704 -0.042430 -0.063148 -0.069241 -0.068755           3  titik  \n",
       "3   -0.070425 -0.044378 -0.065955 -0.071458 -0.070688           4  titik  \n",
       "4   -0.065806 -0.039263 -0.058188 -0.062696 -0.061009           5  titik  \n",
       "..        ...       ...       ...       ...       ...         ...    ...  \n",
       "110 -0.054343 -0.026302 -0.041809 -0.043875 -0.042228         111  titik  \n",
       "111 -0.054408 -0.033516 -0.047402 -0.053279 -0.055458         112  titik  \n",
       "112 -0.067588 -0.038103 -0.053902 -0.059766 -0.061661         113  titik  \n",
       "113 -0.065302 -0.039899 -0.056716 -0.063922 -0.066518         114  titik  \n",
       "114 -0.068234 -0.041803 -0.060606 -0.066032 -0.066580         115  titik  \n",
       "\n",
       "[115 rows x 65 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('csv_separated/angka/titik/pure.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b64fbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>Z13</th>\n",
       "      <th>Z14</th>\n",
       "      <th>Z15</th>\n",
       "      <th>Z16</th>\n",
       "      <th>Z17</th>\n",
       "      <th>Z18</th>\n",
       "      <th>Z19</th>\n",
       "      <th>Z20</th>\n",
       "      <th>Sample Num</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262496</td>\n",
       "      <td>0.517449</td>\n",
       "      <td>0.727365</td>\n",
       "      <td>0.932612</td>\n",
       "      <td>0.393235</td>\n",
       "      <td>0.650800</td>\n",
       "      <td>0.842551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.262918</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057151</td>\n",
       "      <td>-0.089256</td>\n",
       "      <td>-0.074899</td>\n",
       "      <td>-0.058334</td>\n",
       "      <td>-0.070132</td>\n",
       "      <td>-0.088469</td>\n",
       "      <td>-0.077601</td>\n",
       "      <td>-0.066305</td>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258327</td>\n",
       "      <td>0.518769</td>\n",
       "      <td>0.732461</td>\n",
       "      <td>0.938293</td>\n",
       "      <td>0.400590</td>\n",
       "      <td>0.659534</td>\n",
       "      <td>0.850493</td>\n",
       "      <td>1.009012</td>\n",
       "      <td>0.266458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057848</td>\n",
       "      <td>-0.089363</td>\n",
       "      <td>-0.074848</td>\n",
       "      <td>-0.058618</td>\n",
       "      <td>-0.070955</td>\n",
       "      <td>-0.088610</td>\n",
       "      <td>-0.077117</td>\n",
       "      <td>-0.065674</td>\n",
       "      <td>2</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274816</td>\n",
       "      <td>0.536315</td>\n",
       "      <td>0.752159</td>\n",
       "      <td>0.959339</td>\n",
       "      <td>0.424794</td>\n",
       "      <td>0.685993</td>\n",
       "      <td>0.879157</td>\n",
       "      <td>1.036404</td>\n",
       "      <td>0.290968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061454</td>\n",
       "      <td>-0.093907</td>\n",
       "      <td>-0.078565</td>\n",
       "      <td>-0.061478</td>\n",
       "      <td>-0.074942</td>\n",
       "      <td>-0.093178</td>\n",
       "      <td>-0.081417</td>\n",
       "      <td>-0.069630</td>\n",
       "      <td>3</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262680</td>\n",
       "      <td>0.530018</td>\n",
       "      <td>0.743118</td>\n",
       "      <td>0.945634</td>\n",
       "      <td>0.420651</td>\n",
       "      <td>0.681977</td>\n",
       "      <td>0.869163</td>\n",
       "      <td>1.021759</td>\n",
       "      <td>0.293636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059761</td>\n",
       "      <td>-0.091205</td>\n",
       "      <td>-0.077570</td>\n",
       "      <td>-0.062268</td>\n",
       "      <td>-0.071356</td>\n",
       "      <td>-0.088959</td>\n",
       "      <td>-0.078023</td>\n",
       "      <td>-0.067256</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243609</td>\n",
       "      <td>0.507713</td>\n",
       "      <td>0.718878</td>\n",
       "      <td>0.923239</td>\n",
       "      <td>0.392374</td>\n",
       "      <td>0.652716</td>\n",
       "      <td>0.841197</td>\n",
       "      <td>0.996312</td>\n",
       "      <td>0.267018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056987</td>\n",
       "      <td>-0.088836</td>\n",
       "      <td>-0.075183</td>\n",
       "      <td>-0.059511</td>\n",
       "      <td>-0.068158</td>\n",
       "      <td>-0.086596</td>\n",
       "      <td>-0.076351</td>\n",
       "      <td>-0.065696</td>\n",
       "      <td>5</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4978</th>\n",
       "      <td>0.259353</td>\n",
       "      <td>0.050070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107602</td>\n",
       "      <td>0.208105</td>\n",
       "      <td>0.293042</td>\n",
       "      <td>0.337817</td>\n",
       "      <td>0.358921</td>\n",
       "      <td>0.377701</td>\n",
       "      <td>0.469353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.002784</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.004013</td>\n",
       "      <td>125</td>\n",
       "      <td>lihat1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>0.250245</td>\n",
       "      <td>0.022892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107176</td>\n",
       "      <td>0.212584</td>\n",
       "      <td>0.208738</td>\n",
       "      <td>0.315439</td>\n",
       "      <td>0.378562</td>\n",
       "      <td>0.425505</td>\n",
       "      <td>0.394287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025437</td>\n",
       "      <td>0.041651</td>\n",
       "      <td>0.049387</td>\n",
       "      <td>0.053944</td>\n",
       "      <td>0.024894</td>\n",
       "      <td>0.043140</td>\n",
       "      <td>0.053693</td>\n",
       "      <td>0.060905</td>\n",
       "      <td>126</td>\n",
       "      <td>lihat1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>0.274810</td>\n",
       "      <td>0.066562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077998</td>\n",
       "      <td>0.182445</td>\n",
       "      <td>0.231311</td>\n",
       "      <td>0.347752</td>\n",
       "      <td>0.410349</td>\n",
       "      <td>0.454436</td>\n",
       "      <td>0.401479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008082</td>\n",
       "      <td>-0.005477</td>\n",
       "      <td>-0.002638</td>\n",
       "      <td>-0.002016</td>\n",
       "      <td>-0.007925</td>\n",
       "      <td>-0.005930</td>\n",
       "      <td>-0.003743</td>\n",
       "      <td>-0.001941</td>\n",
       "      <td>127</td>\n",
       "      <td>lihat1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>0.253092</td>\n",
       "      <td>0.062023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092041</td>\n",
       "      <td>0.218448</td>\n",
       "      <td>0.205058</td>\n",
       "      <td>0.316318</td>\n",
       "      <td>0.380841</td>\n",
       "      <td>0.429534</td>\n",
       "      <td>0.366944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009190</td>\n",
       "      <td>-0.009736</td>\n",
       "      <td>-0.007093</td>\n",
       "      <td>-0.005440</td>\n",
       "      <td>-0.010266</td>\n",
       "      <td>-0.010199</td>\n",
       "      <td>-0.007267</td>\n",
       "      <td>-0.004349</td>\n",
       "      <td>128</td>\n",
       "      <td>lihat1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>0.250388</td>\n",
       "      <td>0.085920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054668</td>\n",
       "      <td>0.183202</td>\n",
       "      <td>0.170640</td>\n",
       "      <td>0.255449</td>\n",
       "      <td>0.303213</td>\n",
       "      <td>0.347309</td>\n",
       "      <td>0.322242</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031574</td>\n",
       "      <td>-0.044952</td>\n",
       "      <td>-0.039489</td>\n",
       "      <td>-0.031553</td>\n",
       "      <td>-0.033839</td>\n",
       "      <td>-0.041087</td>\n",
       "      <td>-0.034269</td>\n",
       "      <td>-0.026049</td>\n",
       "      <td>129</td>\n",
       "      <td>lihat1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4983 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X0        X1        X2        X3        X4        X5        X6  \\\n",
       "0     0.000000  0.262496  0.517449  0.727365  0.932612  0.393235  0.650800   \n",
       "1     0.000000  0.258327  0.518769  0.732461  0.938293  0.400590  0.659534   \n",
       "2     0.000000  0.274816  0.536315  0.752159  0.959339  0.424794  0.685993   \n",
       "3     0.000000  0.262680  0.530018  0.743118  0.945634  0.420651  0.681977   \n",
       "4     0.000000  0.243609  0.507713  0.718878  0.923239  0.392374  0.652716   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4978  0.259353  0.050070  0.000000  0.107602  0.208105  0.293042  0.337817   \n",
       "4979  0.250245  0.022892  0.000000  0.107176  0.212584  0.208738  0.315439   \n",
       "4980  0.274810  0.066562  0.000000  0.077998  0.182445  0.231311  0.347752   \n",
       "4981  0.253092  0.062023  0.000000  0.092041  0.218448  0.205058  0.316318   \n",
       "4982  0.250388  0.085920  0.000000  0.054668  0.183202  0.170640  0.255449   \n",
       "\n",
       "            X7        X8        X9  ...       Z13       Z14       Z15  \\\n",
       "0     0.842551  1.000000  0.262918  ... -0.057151 -0.089256 -0.074899   \n",
       "1     0.850493  1.009012  0.266458  ... -0.057848 -0.089363 -0.074848   \n",
       "2     0.879157  1.036404  0.290968  ... -0.061454 -0.093907 -0.078565   \n",
       "3     0.869163  1.021759  0.293636  ... -0.059761 -0.091205 -0.077570   \n",
       "4     0.841197  0.996312  0.267018  ... -0.056987 -0.088836 -0.075183   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4978  0.358921  0.377701  0.469353  ...  0.001035  0.001243  0.001868   \n",
       "4979  0.378562  0.425505  0.394287  ...  0.025437  0.041651  0.049387   \n",
       "4980  0.410349  0.454436  0.401479  ... -0.008082 -0.005477 -0.002638   \n",
       "4981  0.380841  0.429534  0.366944  ... -0.009190 -0.009736 -0.007093   \n",
       "4982  0.303213  0.347309  0.322242  ... -0.031574 -0.044952 -0.039489   \n",
       "\n",
       "           Z16       Z17       Z18       Z19       Z20  Sample Num   Label  \n",
       "0    -0.058334 -0.070132 -0.088469 -0.077601 -0.066305           1       g  \n",
       "1    -0.058618 -0.070955 -0.088610 -0.077117 -0.065674           2       g  \n",
       "2    -0.061478 -0.074942 -0.093178 -0.081417 -0.069630           3       g  \n",
       "3    -0.062268 -0.071356 -0.088959 -0.078023 -0.067256           4       g  \n",
       "4    -0.059511 -0.068158 -0.086596 -0.076351 -0.065696           5       g  \n",
       "...        ...       ...       ...       ...       ...         ...     ...  \n",
       "4978  0.002784  0.003031  0.002410  0.002608  0.004013         125  lihat1  \n",
       "4979  0.053944  0.024894  0.043140  0.053693  0.060905         126  lihat1  \n",
       "4980 -0.002016 -0.007925 -0.005930 -0.003743 -0.001941         127  lihat1  \n",
       "4981 -0.005440 -0.010266 -0.010199 -0.007267 -0.004349         128  lihat1  \n",
       "4982 -0.031553 -0.033839 -0.041087 -0.034269 -0.026049         129  lihat1  \n",
       "\n",
       "[4983 rows x 65 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02adaed6",
   "metadata": {},
   "source": [
    "## FUNGSI NORMALISASI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2d4fe5e5-9b9c-4dd2-a93e-29df47da8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisasi(data):\n",
    "    dmin, dmax = np.min(data), np.max(data)\n",
    "    # normalisasi = ((data - dmin) / (dmax - dmin)) * (d2max - d2min) + d2min\n",
    "    normalisasi = data - dmin\n",
    "    return normalisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b4a7fa-f23f-411f-9380-f5d14393c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_points(points, new_x_max):\n",
    "    \"\"\"\n",
    "    Melakukan transformasi skala pada kumpulan titik berdasarkan nilai maksimum baru untuk sumbu X.\n",
    "    \n",
    "    Parameters:\n",
    "        points (numpy.ndarray): Array 2D berisi koordinat titik, dengan kolom pertama sebagai X dan kedua sebagai Y.\n",
    "        new_x_max (float): Nilai maksimum baru untuk sumbu X setelah transformasi.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Array 2D dari titik yang telah ditransformasi.\n",
    "    \"\"\"\n",
    "    # Nilai maksimum awal untuk X\n",
    "    x_max_original = np.max(points[:, 0])\n",
    "    \n",
    "    # Hitung skala\n",
    "    scale = new_x_max / x_max_original\n",
    "    \n",
    "    # Transformasi titik berdasarkan skala\n",
    "    transformed_points = (points * scale)\n",
    "    \n",
    "    return transformed_points[:,0],transformed_points[:,1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583f28e-a900-4031-b592-8003ee1f251a",
   "metadata": {},
   "source": [
    "## EXTRACT DATA WITH NORMALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c51fc8-1259-480c-be3b-04827ddc7b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745946858.132600  366392 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1745946858.134345  367301 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) Graphics (RPL-S)\n",
      "W0000 00:00:1745946858.157290  367303 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745946858.170260  367322 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "import numpy as np\n",
    "# Direktori gambar\n",
    "\n",
    "\n",
    "# Inisialisasi MediaPipe HandLandmarker\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    num_hands=2,\n",
    "    running_mode=vision.RunningMode.IMAGE\n",
    ")\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "max_images = 38\n",
    "path = 'img'\n",
    "\n",
    "# Data untuk DataFrame\n",
    "folders = ['a']\n",
    "folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f)) and not f.startswith('.') ] and f in list('a'))\n",
    "data = []\n",
    "for target in folders:\n",
    "    data_separated = []\n",
    "    image_directory = \"img/\"+target\n",
    "    img_path = sorted([f for f in os.listdir(image_directory) if f.endswith(('.jpg', '.png'))],key=lambda x: int(x.split('.')[0])) #FULL\n",
    "    # img_path = [sorted([f for f in os.listdir(image_directory) if f.endswith(('.jpg', '.png'))],key=lambda x: int(x.split('.')[0]))[i] for i in [2, 40, 70,112]]\n",
    "    for img in img_path:\n",
    "        image_path = os.path.join(image_directory, img)\n",
    "        image = cv2.imread(image_path )\n",
    "        \n",
    "        if image is None:\n",
    "            print(f\"Gagal membuka {image_path } i\")\n",
    "            continue\n",
    "    \n",
    "        # Konversi ke RGB untuk MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "    \n",
    "        # Deteksi tangan\n",
    "        detection_result = detector.detect(mp_image)\n",
    "        if not detection_result.hand_landmarks:\n",
    "            print(f\"Tidak ada tangan terdeteksi dalam {image_path } in label {target} \")\n",
    "            os.remove(image_path)\n",
    "         \n",
    "        \n",
    "        # Format data sesuai permintaan\n",
    "        for hand_idx, hand in enumerate(detection_result.hand_landmarks):\n",
    "            landmark_points = []  # Menyimpan titik untuk koneksi garis\n",
    "\n",
    "            # Ekstrak semua titik koordinat\n",
    "            nilai_X = np.array([landmark.x for landmark in hand])\n",
    "            nilai_Y = np.array([landmark.y for landmark in hand])\n",
    "            nilai_Z = np.array([landmark.y for landmark in hand])\n",
    "            newX = normalisasi(nilai_X,(np.max(nilai_X)-np.min(nilai_X))) \n",
    "            newY = normalisasi(nilai_Y,(np.max(nilai_Y)-np.min(nilai_Y)))\n",
    "            newXY = np.column_stack((newX, newY) )  # Simpan dalam list koordinat int\n",
    "                \n",
    "            newX , newY = scale_points(newXY,10)\n",
    "   \n",
    " \n",
    "        \n",
    "            # Buat dictionary untuk DataFrame dengan format X1, X2,..., Y1, Y2,..., Z\n",
    "            row_data = {f\"X{idx}\": x for idx, x in zip(range(21),newX )}\n",
    "            row_data.update({f\"Y{idx}\": y for idx, y in zip(range(21),newY)})\n",
    "            row_data.update({f\"Z{idx}\": y for idx, y in zip(range(21),nilai_Z)})\n",
    "            # row_data.update({f\"Xc\": np.mean(newX)})\n",
    "            # row_data.update({f\"Yc\": np.mean(newY)})\n",
    "            row_data.update({f\"Z{idx}\": y for idx, y in zip(range(0,21),y_coords)})\n",
    "            # row_data[\"Z\"] = z_coords[0] if z_coords else None  # Ambil satu nilai Z sebagai referensi\n",
    "            # row_data.update({f\"Z{idx+1}\": y for idx, y in enumerate(y_coords)}) \n",
    "            row_data[\"Label\"] = target\n",
    "          # Label tetap \"a\"\n",
    "            data.append(row_data)\n",
    "            # data.append(row_data)\n",
    "            data_separated.append(row_data)\n",
    "        directory_goal = f'csv_separated/{target}'\n",
    "        if not os.path.exists(directory_goal):\n",
    "            os.makedirs(directory_goal)\n",
    "        pd.DataFrame(data_separated).to_csv(f'{directory_goal}/filter.csv', index=False)\n",
    "\n",
    "        \n",
    "    print('next')\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cf679564",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'csv/static/pure.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd92ad8",
   "metadata": {},
   "source": [
    "## Preprocessing data pada csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cafcddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Membaca data\n",
    "df = pd.read_csv(PATH)\n",
    "\n",
    "def normalisasi_per_baris(data, d2max=10, d2min=0):\n",
    "    # Mencari nilai min dan max untuk setiap baris\n",
    "    dmin = np.min(data, axis=1)  # Minimum untuk setiap baris\n",
    "    dmax = np.max(data, axis=1)  # Maximum untuk setiap baris\n",
    "    \n",
    "    # Normalisasi per baris\n",
    "    norm_data = (data - dmin[:, np.newaxis]) \n",
    "    \n",
    "    # Skala hasil normalisasi ke rentang [d2min, d2max] yang diinginkan\n",
    "    return norm_data\n",
    "\n",
    "# Fungsi penskalaan (scaling) untuk points 2D (vektor)\n",
    "def scale_points(points, new_x_max,x):\n",
    "    # Skala berdasarkan nilai maksimum X yang baru\n",
    "    x_max_original = np.max(x)  # Nilai maksimum awal untuk X\n",
    "    scale = new_x_max / x_max_original  # Hitung skala\n",
    "    transformed_points = points * scale  # Transformasi titik berdasarkan skala\n",
    "    return transformed_points\n",
    "\n",
    "# Proses normalisasi dan penskalaan pada seluruh dataframe\n",
    "def process_hand_data_v2(df, new_x_max=1):\n",
    "    # Ekstrak kolom X dan Y (kolom X0-X20, Y0-Y20)\n",
    "    x_cols = [col for col in df.columns if col.startswith('X')]\n",
    "    y_cols = [col for col in df.columns if col.startswith('Y')]\n",
    "    \n",
    "    # Ambil semua nilai X dan Y sebagai array 2D\n",
    "    nilai_X = df[x_cols].values\n",
    "    nilai_Y = df[y_cols].values\n",
    "    \n",
    "    # Normalisasi per baris untuk X dan Y\n",
    "    newX = normalisasi_per_baris(nilai_X, new_x_max)  # Rentang akhir 10 untuk X\n",
    "    newY = normalisasi_per_baris(nilai_Y, new_x_max)  # Rentang akhir 10 untuk Y\n",
    "   \n",
    "    # Gabungkan koordinat X dan Y setelah dinormalisasi\n",
    "    newXY = np.column_stack((newX, newY))\n",
    "  \n",
    "    # Skalakan titik untuk memastikan X berada dalam rentang [0, 10]\n",
    "    scaled_points = scale_points(newXY, new_x_max,newX[0,:])\n",
    "    \n",
    "    # Menyimpan hasil normalisasi dan penskalaan kembali ke DataFrame\n",
    "    df[x_cols] = scaled_points[:, :len(x_cols)]\n",
    "    df[y_cols] = scaled_points[:, len(x_cols):]\n",
    "    \n",
    "    return df\n",
    "# Proses DataFrame dengan fungsi yang lebih efisien\n",
    "df = process_hand_data_v2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bdec8d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'csv/static/filter.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f1682e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>Z13</th>\n",
       "      <th>Z14</th>\n",
       "      <th>Z15</th>\n",
       "      <th>Z16</th>\n",
       "      <th>Z17</th>\n",
       "      <th>Z18</th>\n",
       "      <th>Z19</th>\n",
       "      <th>Z20</th>\n",
       "      <th>Sample Num</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.277589</td>\n",
       "      <td>0.707599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.845149</td>\n",
       "      <td>0.492069</td>\n",
       "      <td>0.805588</td>\n",
       "      <td>0.767959</td>\n",
       "      <td>0.785418</td>\n",
       "      <td>0.791802</td>\n",
       "      <td>0.520463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027655</td>\n",
       "      <td>-0.061845</td>\n",
       "      <td>-0.075178</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.037640</td>\n",
       "      <td>-0.060164</td>\n",
       "      <td>-0.067015</td>\n",
       "      <td>-0.067270</td>\n",
       "      <td>1</td>\n",
       "      <td>titik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.292780</td>\n",
       "      <td>0.706283</td>\n",
       "      <td>1.006615</td>\n",
       "      <td>0.892223</td>\n",
       "      <td>0.565120</td>\n",
       "      <td>0.813602</td>\n",
       "      <td>0.785027</td>\n",
       "      <td>0.818274</td>\n",
       "      <td>0.813880</td>\n",
       "      <td>0.530407</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026346</td>\n",
       "      <td>-0.057128</td>\n",
       "      <td>-0.068359</td>\n",
       "      <td>-0.067404</td>\n",
       "      <td>-0.037278</td>\n",
       "      <td>-0.056616</td>\n",
       "      <td>-0.061879</td>\n",
       "      <td>-0.060964</td>\n",
       "      <td>2</td>\n",
       "      <td>titik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.314016</td>\n",
       "      <td>0.705340</td>\n",
       "      <td>1.022556</td>\n",
       "      <td>0.956574</td>\n",
       "      <td>0.645630</td>\n",
       "      <td>0.805853</td>\n",
       "      <td>0.776126</td>\n",
       "      <td>0.829578</td>\n",
       "      <td>0.820300</td>\n",
       "      <td>0.526215</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031913</td>\n",
       "      <td>-0.061052</td>\n",
       "      <td>-0.070387</td>\n",
       "      <td>-0.068704</td>\n",
       "      <td>-0.042430</td>\n",
       "      <td>-0.063148</td>\n",
       "      <td>-0.069241</td>\n",
       "      <td>-0.068755</td>\n",
       "      <td>3</td>\n",
       "      <td>titik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.318516</td>\n",
       "      <td>0.707208</td>\n",
       "      <td>1.024506</td>\n",
       "      <td>0.984650</td>\n",
       "      <td>0.666793</td>\n",
       "      <td>0.805961</td>\n",
       "      <td>0.770219</td>\n",
       "      <td>0.827068</td>\n",
       "      <td>0.830426</td>\n",
       "      <td>0.523206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033412</td>\n",
       "      <td>-0.064247</td>\n",
       "      <td>-0.072699</td>\n",
       "      <td>-0.070425</td>\n",
       "      <td>-0.044378</td>\n",
       "      <td>-0.065955</td>\n",
       "      <td>-0.071458</td>\n",
       "      <td>-0.070688</td>\n",
       "      <td>4</td>\n",
       "      <td>titik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.319794</td>\n",
       "      <td>0.713486</td>\n",
       "      <td>1.004819</td>\n",
       "      <td>0.902697</td>\n",
       "      <td>0.574729</td>\n",
       "      <td>0.798964</td>\n",
       "      <td>0.765606</td>\n",
       "      <td>0.804889</td>\n",
       "      <td>0.809297</td>\n",
       "      <td>0.520148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028226</td>\n",
       "      <td>-0.058030</td>\n",
       "      <td>-0.067803</td>\n",
       "      <td>-0.065806</td>\n",
       "      <td>-0.039263</td>\n",
       "      <td>-0.058188</td>\n",
       "      <td>-0.062696</td>\n",
       "      <td>-0.061009</td>\n",
       "      <td>5</td>\n",
       "      <td>titik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>0.483934</td>\n",
       "      <td>0.840272</td>\n",
       "      <td>1.107165</td>\n",
       "      <td>1.218978</td>\n",
       "      <td>1.271879</td>\n",
       "      <td>0.873357</td>\n",
       "      <td>0.999848</td>\n",
       "      <td>1.052778</td>\n",
       "      <td>1.093514</td>\n",
       "      <td>0.655411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019464</td>\n",
       "      <td>-0.035298</td>\n",
       "      <td>-0.052188</td>\n",
       "      <td>-0.064645</td>\n",
       "      <td>-0.024087</td>\n",
       "      <td>-0.042630</td>\n",
       "      <td>-0.055679</td>\n",
       "      <td>-0.064658</td>\n",
       "      <td>106</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>0.345708</td>\n",
       "      <td>0.717917</td>\n",
       "      <td>1.007379</td>\n",
       "      <td>1.150110</td>\n",
       "      <td>1.204503</td>\n",
       "      <td>0.803884</td>\n",
       "      <td>0.950948</td>\n",
       "      <td>1.018745</td>\n",
       "      <td>1.073571</td>\n",
       "      <td>0.591292</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004732</td>\n",
       "      <td>-0.015818</td>\n",
       "      <td>-0.032296</td>\n",
       "      <td>-0.044682</td>\n",
       "      <td>-0.007350</td>\n",
       "      <td>-0.022959</td>\n",
       "      <td>-0.035271</td>\n",
       "      <td>-0.043746</td>\n",
       "      <td>107</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>0.434926</td>\n",
       "      <td>0.819972</td>\n",
       "      <td>1.095845</td>\n",
       "      <td>1.228584</td>\n",
       "      <td>1.296090</td>\n",
       "      <td>0.880498</td>\n",
       "      <td>1.050965</td>\n",
       "      <td>1.141648</td>\n",
       "      <td>1.205808</td>\n",
       "      <td>0.662672</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012812</td>\n",
       "      <td>-0.026452</td>\n",
       "      <td>-0.043490</td>\n",
       "      <td>-0.055718</td>\n",
       "      <td>-0.017742</td>\n",
       "      <td>-0.036186</td>\n",
       "      <td>-0.050320</td>\n",
       "      <td>-0.059872</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>0.255808</td>\n",
       "      <td>0.651656</td>\n",
       "      <td>0.965841</td>\n",
       "      <td>1.108683</td>\n",
       "      <td>1.167003</td>\n",
       "      <td>0.765934</td>\n",
       "      <td>0.916647</td>\n",
       "      <td>0.994213</td>\n",
       "      <td>1.055820</td>\n",
       "      <td>0.551157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010628</td>\n",
       "      <td>-0.023872</td>\n",
       "      <td>-0.040453</td>\n",
       "      <td>-0.052721</td>\n",
       "      <td>-0.016467</td>\n",
       "      <td>-0.034047</td>\n",
       "      <td>-0.046766</td>\n",
       "      <td>-0.055560</td>\n",
       "      <td>109</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>0.277161</td>\n",
       "      <td>0.676166</td>\n",
       "      <td>0.978212</td>\n",
       "      <td>1.118666</td>\n",
       "      <td>1.159758</td>\n",
       "      <td>0.778061</td>\n",
       "      <td>0.912729</td>\n",
       "      <td>0.974156</td>\n",
       "      <td>1.023002</td>\n",
       "      <td>0.569924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006432</td>\n",
       "      <td>-0.020297</td>\n",
       "      <td>-0.038867</td>\n",
       "      <td>-0.052142</td>\n",
       "      <td>-0.011757</td>\n",
       "      <td>-0.029727</td>\n",
       "      <td>-0.043534</td>\n",
       "      <td>-0.052970</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1665 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X0        X1        X2        X3        X4        X5        X6  \\\n",
       "0     0.277589  0.707599  1.000000  0.845149  0.492069  0.805588  0.767959   \n",
       "1     0.292780  0.706283  1.006615  0.892223  0.565120  0.813602  0.785027   \n",
       "2     0.314016  0.705340  1.022556  0.956574  0.645630  0.805853  0.776126   \n",
       "3     0.318516  0.707208  1.024506  0.984650  0.666793  0.805961  0.770219   \n",
       "4     0.319794  0.713486  1.004819  0.902697  0.574729  0.798964  0.765606   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1660  0.483934  0.840272  1.107165  1.218978  1.271879  0.873357  0.999848   \n",
       "1661  0.345708  0.717917  1.007379  1.150110  1.204503  0.803884  0.950948   \n",
       "1662  0.434926  0.819972  1.095845  1.228584  1.296090  0.880498  1.050965   \n",
       "1663  0.255808  0.651656  0.965841  1.108683  1.167003  0.765934  0.916647   \n",
       "1664  0.277161  0.676166  0.978212  1.118666  1.159758  0.778061  0.912729   \n",
       "\n",
       "            X7        X8        X9  ...       Z13       Z14       Z15  \\\n",
       "0     0.785418  0.791802  0.520463  ... -0.027655 -0.061845 -0.075178   \n",
       "1     0.818274  0.813880  0.530407  ... -0.026346 -0.057128 -0.068359   \n",
       "2     0.829578  0.820300  0.526215  ... -0.031913 -0.061052 -0.070387   \n",
       "3     0.827068  0.830426  0.523206  ... -0.033412 -0.064247 -0.072699   \n",
       "4     0.804889  0.809297  0.520148  ... -0.028226 -0.058030 -0.067803   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1660  1.052778  1.093514  0.655411  ... -0.019464 -0.035298 -0.052188   \n",
       "1661  1.018745  1.073571  0.591292  ... -0.004732 -0.015818 -0.032296   \n",
       "1662  1.141648  1.205808  0.662672  ... -0.012812 -0.026452 -0.043490   \n",
       "1663  0.994213  1.055820  0.551157  ... -0.010628 -0.023872 -0.040453   \n",
       "1664  0.974156  1.023002  0.569924  ... -0.006432 -0.020297 -0.038867   \n",
       "\n",
       "           Z16       Z17       Z18       Z19       Z20  Sample Num  Label  \n",
       "0    -0.075484 -0.037640 -0.060164 -0.067015 -0.067270           1  titik  \n",
       "1    -0.067404 -0.037278 -0.056616 -0.061879 -0.060964           2  titik  \n",
       "2    -0.068704 -0.042430 -0.063148 -0.069241 -0.068755           3  titik  \n",
       "3    -0.070425 -0.044378 -0.065955 -0.071458 -0.070688           4  titik  \n",
       "4    -0.065806 -0.039263 -0.058188 -0.062696 -0.061009           5  titik  \n",
       "...        ...       ...       ...       ...       ...         ...    ...  \n",
       "1660 -0.064645 -0.024087 -0.042630 -0.055679 -0.064658         106      5  \n",
       "1661 -0.044682 -0.007350 -0.022959 -0.035271 -0.043746         107      5  \n",
       "1662 -0.055718 -0.017742 -0.036186 -0.050320 -0.059872         108      5  \n",
       "1663 -0.052721 -0.016467 -0.034047 -0.046766 -0.055560         109      5  \n",
       "1664 -0.052142 -0.011757 -0.029727 -0.043534 -0.052970         110      5  \n",
       "\n",
       "[1665 rows x 65 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2b961fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X14</th>\n",
       "      <th>X16</th>\n",
       "      <th>...</th>\n",
       "      <th>Y16</th>\n",
       "      <th>Y17</th>\n",
       "      <th>Y19</th>\n",
       "      <th>Y20</th>\n",
       "      <th>Z4</th>\n",
       "      <th>Z5</th>\n",
       "      <th>Z8</th>\n",
       "      <th>Z12</th>\n",
       "      <th>Z20</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>0.620412</td>\n",
       "      <td>0.553800</td>\n",
       "      <td>0.609672</td>\n",
       "      <td>0.710086</td>\n",
       "      <td>0.774501</td>\n",
       "      <td>0.459617</td>\n",
       "      <td>0.416129</td>\n",
       "      <td>0.359708</td>\n",
       "      <td>0.314228</td>\n",
       "      <td>0.250985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.779183</td>\n",
       "      <td>0.482352</td>\n",
       "      <td>0.798444</td>\n",
       "      <td>0.785335</td>\n",
       "      <td>-0.076675</td>\n",
       "      <td>-0.034008</td>\n",
       "      <td>-0.081585</td>\n",
       "      <td>-0.073792</td>\n",
       "      <td>-0.066263</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>0.648044</td>\n",
       "      <td>0.550161</td>\n",
       "      <td>0.609679</td>\n",
       "      <td>0.695170</td>\n",
       "      <td>0.746855</td>\n",
       "      <td>0.453927</td>\n",
       "      <td>0.428624</td>\n",
       "      <td>0.382615</td>\n",
       "      <td>0.304007</td>\n",
       "      <td>0.267301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800786</td>\n",
       "      <td>0.486136</td>\n",
       "      <td>0.806741</td>\n",
       "      <td>0.791544</td>\n",
       "      <td>-0.065024</td>\n",
       "      <td>-0.030589</td>\n",
       "      <td>-0.075569</td>\n",
       "      <td>-0.069992</td>\n",
       "      <td>-0.064648</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>0.655857</td>\n",
       "      <td>0.537875</td>\n",
       "      <td>0.591483</td>\n",
       "      <td>0.647204</td>\n",
       "      <td>0.677048</td>\n",
       "      <td>0.437597</td>\n",
       "      <td>0.420456</td>\n",
       "      <td>0.381831</td>\n",
       "      <td>0.289051</td>\n",
       "      <td>0.261707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801509</td>\n",
       "      <td>0.510402</td>\n",
       "      <td>0.823028</td>\n",
       "      <td>0.796726</td>\n",
       "      <td>-0.053079</td>\n",
       "      <td>-0.031246</td>\n",
       "      <td>-0.076454</td>\n",
       "      <td>-0.066159</td>\n",
       "      <td>-0.064970</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>0.632252</td>\n",
       "      <td>0.484146</td>\n",
       "      <td>0.556823</td>\n",
       "      <td>0.578507</td>\n",
       "      <td>0.582169</td>\n",
       "      <td>0.404069</td>\n",
       "      <td>0.392760</td>\n",
       "      <td>0.369413</td>\n",
       "      <td>0.260457</td>\n",
       "      <td>0.263092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821580</td>\n",
       "      <td>0.547542</td>\n",
       "      <td>0.856962</td>\n",
       "      <td>0.823616</td>\n",
       "      <td>-0.051304</td>\n",
       "      <td>-0.028386</td>\n",
       "      <td>-0.067759</td>\n",
       "      <td>-0.058877</td>\n",
       "      <td>-0.056110</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>0.622380</td>\n",
       "      <td>0.456169</td>\n",
       "      <td>0.553759</td>\n",
       "      <td>0.563140</td>\n",
       "      <td>0.553748</td>\n",
       "      <td>0.385855</td>\n",
       "      <td>0.389253</td>\n",
       "      <td>0.378859</td>\n",
       "      <td>0.247346</td>\n",
       "      <td>0.267470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.827879</td>\n",
       "      <td>0.549060</td>\n",
       "      <td>0.858603</td>\n",
       "      <td>0.820202</td>\n",
       "      <td>-0.046947</td>\n",
       "      <td>-0.029272</td>\n",
       "      <td>-0.070310</td>\n",
       "      <td>-0.061880</td>\n",
       "      <td>-0.060005</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>0.518665</td>\n",
       "      <td>0.468715</td>\n",
       "      <td>0.464519</td>\n",
       "      <td>0.561599</td>\n",
       "      <td>0.632803</td>\n",
       "      <td>0.363047</td>\n",
       "      <td>0.366153</td>\n",
       "      <td>0.333443</td>\n",
       "      <td>0.278241</td>\n",
       "      <td>0.250242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.601390</td>\n",
       "      <td>0.399593</td>\n",
       "      <td>0.625273</td>\n",
       "      <td>0.612246</td>\n",
       "      <td>-0.040683</td>\n",
       "      <td>-0.023582</td>\n",
       "      <td>-0.057727</td>\n",
       "      <td>-0.057446</td>\n",
       "      <td>-0.054615</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>0.479592</td>\n",
       "      <td>0.492402</td>\n",
       "      <td>0.460428</td>\n",
       "      <td>0.567847</td>\n",
       "      <td>0.646506</td>\n",
       "      <td>0.363978</td>\n",
       "      <td>0.366405</td>\n",
       "      <td>0.335657</td>\n",
       "      <td>0.272826</td>\n",
       "      <td>0.243566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572175</td>\n",
       "      <td>0.362705</td>\n",
       "      <td>0.586762</td>\n",
       "      <td>0.576003</td>\n",
       "      <td>-0.046307</td>\n",
       "      <td>-0.024648</td>\n",
       "      <td>-0.063631</td>\n",
       "      <td>-0.059318</td>\n",
       "      <td>-0.051593</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>0.497071</td>\n",
       "      <td>0.468079</td>\n",
       "      <td>0.473821</td>\n",
       "      <td>0.583719</td>\n",
       "      <td>0.665726</td>\n",
       "      <td>0.374283</td>\n",
       "      <td>0.365041</td>\n",
       "      <td>0.331270</td>\n",
       "      <td>0.278976</td>\n",
       "      <td>0.240757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509559</td>\n",
       "      <td>0.297585</td>\n",
       "      <td>0.533986</td>\n",
       "      <td>0.520996</td>\n",
       "      <td>-0.051850</td>\n",
       "      <td>-0.025143</td>\n",
       "      <td>-0.063662</td>\n",
       "      <td>-0.060158</td>\n",
       "      <td>-0.053554</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>0.508654</td>\n",
       "      <td>0.473224</td>\n",
       "      <td>0.461980</td>\n",
       "      <td>0.554947</td>\n",
       "      <td>0.618435</td>\n",
       "      <td>0.360702</td>\n",
       "      <td>0.363849</td>\n",
       "      <td>0.331207</td>\n",
       "      <td>0.273479</td>\n",
       "      <td>0.254968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642239</td>\n",
       "      <td>0.427127</td>\n",
       "      <td>0.655791</td>\n",
       "      <td>0.657473</td>\n",
       "      <td>-0.050763</td>\n",
       "      <td>-0.026432</td>\n",
       "      <td>-0.063019</td>\n",
       "      <td>-0.060667</td>\n",
       "      <td>-0.054003</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>0.532491</td>\n",
       "      <td>0.472117</td>\n",
       "      <td>0.456917</td>\n",
       "      <td>0.536679</td>\n",
       "      <td>0.591651</td>\n",
       "      <td>0.363465</td>\n",
       "      <td>0.360952</td>\n",
       "      <td>0.322797</td>\n",
       "      <td>0.275863</td>\n",
       "      <td>0.251548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650266</td>\n",
       "      <td>0.475218</td>\n",
       "      <td>0.693163</td>\n",
       "      <td>0.673433</td>\n",
       "      <td>-0.040266</td>\n",
       "      <td>-0.024045</td>\n",
       "      <td>-0.058459</td>\n",
       "      <td>-0.055757</td>\n",
       "      <td>-0.053361</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>229 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X3        X4        X6        X7        X8       X10       X11  \\\n",
       "1272  0.620412  0.553800  0.609672  0.710086  0.774501  0.459617  0.416129   \n",
       "1273  0.648044  0.550161  0.609679  0.695170  0.746855  0.453927  0.428624   \n",
       "1274  0.655857  0.537875  0.591483  0.647204  0.677048  0.437597  0.420456   \n",
       "1275  0.632252  0.484146  0.556823  0.578507  0.582169  0.404069  0.392760   \n",
       "1276  0.622380  0.456169  0.553759  0.563140  0.553748  0.385855  0.389253   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1496  0.518665  0.468715  0.464519  0.561599  0.632803  0.363047  0.366153   \n",
       "1497  0.479592  0.492402  0.460428  0.567847  0.646506  0.363978  0.366405   \n",
       "1498  0.497071  0.468079  0.473821  0.583719  0.665726  0.374283  0.365041   \n",
       "1499  0.508654  0.473224  0.461980  0.554947  0.618435  0.360702  0.363849   \n",
       "1500  0.532491  0.472117  0.456917  0.536679  0.591651  0.363465  0.360952   \n",
       "\n",
       "           X12       X14       X16  ...       Y16       Y17       Y19  \\\n",
       "1272  0.359708  0.314228  0.250985  ...  0.779183  0.482352  0.798444   \n",
       "1273  0.382615  0.304007  0.267301  ...  0.800786  0.486136  0.806741   \n",
       "1274  0.381831  0.289051  0.261707  ...  0.801509  0.510402  0.823028   \n",
       "1275  0.369413  0.260457  0.263092  ...  0.821580  0.547542  0.856962   \n",
       "1276  0.378859  0.247346  0.267470  ...  0.827879  0.549060  0.858603   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1496  0.333443  0.278241  0.250242  ...  0.601390  0.399593  0.625273   \n",
       "1497  0.335657  0.272826  0.243566  ...  0.572175  0.362705  0.586762   \n",
       "1498  0.331270  0.278976  0.240757  ...  0.509559  0.297585  0.533986   \n",
       "1499  0.331207  0.273479  0.254968  ...  0.642239  0.427127  0.655791   \n",
       "1500  0.322797  0.275863  0.251548  ...  0.650266  0.475218  0.693163   \n",
       "\n",
       "           Y20        Z4        Z5        Z8       Z12       Z20  Label  \n",
       "1272  0.785335 -0.076675 -0.034008 -0.081585 -0.073792 -0.066263      z  \n",
       "1273  0.791544 -0.065024 -0.030589 -0.075569 -0.069992 -0.064648      z  \n",
       "1274  0.796726 -0.053079 -0.031246 -0.076454 -0.066159 -0.064970      z  \n",
       "1275  0.823616 -0.051304 -0.028386 -0.067759 -0.058877 -0.056110      z  \n",
       "1276  0.820202 -0.046947 -0.029272 -0.070310 -0.061880 -0.060005      z  \n",
       "...        ...       ...       ...       ...       ...       ...    ...  \n",
       "1496  0.612246 -0.040683 -0.023582 -0.057727 -0.057446 -0.054615      z  \n",
       "1497  0.576003 -0.046307 -0.024648 -0.063631 -0.059318 -0.051593      z  \n",
       "1498  0.520996 -0.051850 -0.025143 -0.063662 -0.060158 -0.053554      z  \n",
       "1499  0.657473 -0.050763 -0.026432 -0.063019 -0.060667 -0.054003      z  \n",
       "1500  0.673433 -0.040266 -0.024045 -0.058459 -0.055757 -0.053361      z  \n",
       "\n",
       "[229 rows x 34 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Label']=='z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0661d67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>Z13</th>\n",
       "      <th>Z14</th>\n",
       "      <th>Z15</th>\n",
       "      <th>Z16</th>\n",
       "      <th>Z17</th>\n",
       "      <th>Z18</th>\n",
       "      <th>Z19</th>\n",
       "      <th>Z20</th>\n",
       "      <th>Sample Num</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.262496</td>\n",
       "      <td>0.517449</td>\n",
       "      <td>0.727365</td>\n",
       "      <td>0.932612</td>\n",
       "      <td>0.393235</td>\n",
       "      <td>0.650800</td>\n",
       "      <td>0.842551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.262918</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057151</td>\n",
       "      <td>-0.089256</td>\n",
       "      <td>-0.074899</td>\n",
       "      <td>-0.058334</td>\n",
       "      <td>-0.070132</td>\n",
       "      <td>-0.088469</td>\n",
       "      <td>-0.077601</td>\n",
       "      <td>-0.066305</td>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258327</td>\n",
       "      <td>0.518769</td>\n",
       "      <td>0.732461</td>\n",
       "      <td>0.938293</td>\n",
       "      <td>0.400590</td>\n",
       "      <td>0.659534</td>\n",
       "      <td>0.850493</td>\n",
       "      <td>1.009012</td>\n",
       "      <td>0.266458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057848</td>\n",
       "      <td>-0.089363</td>\n",
       "      <td>-0.074848</td>\n",
       "      <td>-0.058618</td>\n",
       "      <td>-0.070955</td>\n",
       "      <td>-0.088610</td>\n",
       "      <td>-0.077117</td>\n",
       "      <td>-0.065674</td>\n",
       "      <td>2</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274816</td>\n",
       "      <td>0.536315</td>\n",
       "      <td>0.752159</td>\n",
       "      <td>0.959339</td>\n",
       "      <td>0.424794</td>\n",
       "      <td>0.685993</td>\n",
       "      <td>0.879157</td>\n",
       "      <td>1.036404</td>\n",
       "      <td>0.290968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061454</td>\n",
       "      <td>-0.093907</td>\n",
       "      <td>-0.078565</td>\n",
       "      <td>-0.061478</td>\n",
       "      <td>-0.074942</td>\n",
       "      <td>-0.093178</td>\n",
       "      <td>-0.081417</td>\n",
       "      <td>-0.069630</td>\n",
       "      <td>3</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.262680</td>\n",
       "      <td>0.530018</td>\n",
       "      <td>0.743118</td>\n",
       "      <td>0.945634</td>\n",
       "      <td>0.420651</td>\n",
       "      <td>0.681977</td>\n",
       "      <td>0.869163</td>\n",
       "      <td>1.021759</td>\n",
       "      <td>0.293636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059761</td>\n",
       "      <td>-0.091205</td>\n",
       "      <td>-0.077570</td>\n",
       "      <td>-0.062268</td>\n",
       "      <td>-0.071356</td>\n",
       "      <td>-0.088959</td>\n",
       "      <td>-0.078023</td>\n",
       "      <td>-0.067256</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.243609</td>\n",
       "      <td>0.507713</td>\n",
       "      <td>0.718878</td>\n",
       "      <td>0.923239</td>\n",
       "      <td>0.392374</td>\n",
       "      <td>0.652716</td>\n",
       "      <td>0.841197</td>\n",
       "      <td>0.996312</td>\n",
       "      <td>0.267018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056987</td>\n",
       "      <td>-0.088836</td>\n",
       "      <td>-0.075183</td>\n",
       "      <td>-0.059511</td>\n",
       "      <td>-0.068158</td>\n",
       "      <td>-0.086596</td>\n",
       "      <td>-0.076351</td>\n",
       "      <td>-0.065696</td>\n",
       "      <td>5</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088521</td>\n",
       "      <td>0.238378</td>\n",
       "      <td>0.387401</td>\n",
       "      <td>0.445715</td>\n",
       "      <td>0.412194</td>\n",
       "      <td>0.492168</td>\n",
       "      <td>0.427603</td>\n",
       "      <td>0.363169</td>\n",
       "      <td>0.418116</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004367</td>\n",
       "      <td>-0.002546</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>-0.006921</td>\n",
       "      <td>-0.005112</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>0.003144</td>\n",
       "      <td>76</td>\n",
       "      <td>delete_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5109</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.077284</td>\n",
       "      <td>0.212639</td>\n",
       "      <td>0.373947</td>\n",
       "      <td>0.442660</td>\n",
       "      <td>0.369622</td>\n",
       "      <td>0.489487</td>\n",
       "      <td>0.419909</td>\n",
       "      <td>0.347020</td>\n",
       "      <td>0.387698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007689</td>\n",
       "      <td>-0.006332</td>\n",
       "      <td>-0.002150</td>\n",
       "      <td>-0.000515</td>\n",
       "      <td>-0.008608</td>\n",
       "      <td>-0.007784</td>\n",
       "      <td>-0.003251</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>77</td>\n",
       "      <td>delete_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5110</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042045</td>\n",
       "      <td>0.147402</td>\n",
       "      <td>0.297640</td>\n",
       "      <td>0.385955</td>\n",
       "      <td>0.285512</td>\n",
       "      <td>0.431621</td>\n",
       "      <td>0.365643</td>\n",
       "      <td>0.294203</td>\n",
       "      <td>0.325706</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010225</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.008171</td>\n",
       "      <td>-0.005575</td>\n",
       "      <td>-0.012218</td>\n",
       "      <td>-0.013398</td>\n",
       "      <td>-0.008820</td>\n",
       "      <td>-0.004770</td>\n",
       "      <td>78</td>\n",
       "      <td>delete_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5111</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027985</td>\n",
       "      <td>0.115774</td>\n",
       "      <td>0.250317</td>\n",
       "      <td>0.336940</td>\n",
       "      <td>0.216791</td>\n",
       "      <td>0.393710</td>\n",
       "      <td>0.333120</td>\n",
       "      <td>0.261263</td>\n",
       "      <td>0.266437</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014969</td>\n",
       "      <td>-0.017042</td>\n",
       "      <td>-0.012246</td>\n",
       "      <td>-0.008893</td>\n",
       "      <td>-0.016799</td>\n",
       "      <td>-0.017754</td>\n",
       "      <td>-0.012925</td>\n",
       "      <td>-0.008874</td>\n",
       "      <td>79</td>\n",
       "      <td>delete_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5112</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.070805</td>\n",
       "      <td>0.198071</td>\n",
       "      <td>0.289609</td>\n",
       "      <td>0.204714</td>\n",
       "      <td>0.363397</td>\n",
       "      <td>0.312960</td>\n",
       "      <td>0.242393</td>\n",
       "      <td>0.263717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013237</td>\n",
       "      <td>-0.013975</td>\n",
       "      <td>-0.009839</td>\n",
       "      <td>-0.007923</td>\n",
       "      <td>-0.014047</td>\n",
       "      <td>-0.015136</td>\n",
       "      <td>-0.011826</td>\n",
       "      <td>-0.009138</td>\n",
       "      <td>80</td>\n",
       "      <td>delete_all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5113 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X0        X1        X2        X3        X4        X5        X6  \\\n",
       "0     0.0  0.262496  0.517449  0.727365  0.932612  0.393235  0.650800   \n",
       "1     0.0  0.258327  0.518769  0.732461  0.938293  0.400590  0.659534   \n",
       "2     0.0  0.274816  0.536315  0.752159  0.959339  0.424794  0.685993   \n",
       "3     0.0  0.262680  0.530018  0.743118  0.945634  0.420651  0.681977   \n",
       "4     0.0  0.243609  0.507713  0.718878  0.923239  0.392374  0.652716   \n",
       "...   ...       ...       ...       ...       ...       ...       ...   \n",
       "5108  0.0  0.088521  0.238378  0.387401  0.445715  0.412194  0.492168   \n",
       "5109  0.0  0.077284  0.212639  0.373947  0.442660  0.369622  0.489487   \n",
       "5110  0.0  0.042045  0.147402  0.297640  0.385955  0.285512  0.431621   \n",
       "5111  0.0  0.027985  0.115774  0.250317  0.336940  0.216791  0.393710   \n",
       "5112  0.0  0.002837  0.070805  0.198071  0.289609  0.204714  0.363397   \n",
       "\n",
       "            X7        X8        X9  ...       Z13       Z14       Z15  \\\n",
       "0     0.842551  1.000000  0.262918  ... -0.057151 -0.089256 -0.074899   \n",
       "1     0.850493  1.009012  0.266458  ... -0.057848 -0.089363 -0.074848   \n",
       "2     0.879157  1.036404  0.290968  ... -0.061454 -0.093907 -0.078565   \n",
       "3     0.869163  1.021759  0.293636  ... -0.059761 -0.091205 -0.077570   \n",
       "4     0.841197  0.996312  0.267018  ... -0.056987 -0.088836 -0.075183   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5108  0.427603  0.363169  0.418116  ... -0.004367 -0.002546  0.000341   \n",
       "5109  0.419909  0.347020  0.387698  ... -0.007689 -0.006332 -0.002150   \n",
       "5110  0.365643  0.294203  0.325706  ... -0.010225 -0.011968 -0.008171   \n",
       "5111  0.333120  0.261263  0.266437  ... -0.014969 -0.017042 -0.012246   \n",
       "5112  0.312960  0.242393  0.263717  ... -0.013237 -0.013975 -0.009839   \n",
       "\n",
       "           Z16       Z17       Z18       Z19       Z20  Sample Num       Label  \n",
       "0    -0.058334 -0.070132 -0.088469 -0.077601 -0.066305           1           g  \n",
       "1    -0.058618 -0.070955 -0.088610 -0.077117 -0.065674           2           g  \n",
       "2    -0.061478 -0.074942 -0.093178 -0.081417 -0.069630           3           g  \n",
       "3    -0.062268 -0.071356 -0.088959 -0.078023 -0.067256           4           g  \n",
       "4    -0.059511 -0.068158 -0.086596 -0.076351 -0.065696           5           g  \n",
       "...        ...       ...       ...       ...       ...         ...         ...  \n",
       "5108  0.001100 -0.006921 -0.005112 -0.000347  0.003144          76  delete_all  \n",
       "5109 -0.000515 -0.008608 -0.007784 -0.003251  0.000129          77  delete_all  \n",
       "5110 -0.005575 -0.012218 -0.013398 -0.008820 -0.004770          78  delete_all  \n",
       "5111 -0.008893 -0.016799 -0.017754 -0.012925 -0.008874          79  delete_all  \n",
       "5112 -0.007923 -0.014047 -0.015136 -0.011826 -0.009138          80  delete_all  \n",
       "\n",
       "[5113 rows x 65 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('csv/static/filter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e081d819-500d-411c-9e2e-d9079a5942ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('csv/static/filter.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c9f6a",
   "metadata": {},
   "source": [
    "## DROP KOLOM TERTENTU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f159832-5712-466f-a720-d2b2efc057f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>Z12</th>\n",
       "      <th>Z13</th>\n",
       "      <th>Z14</th>\n",
       "      <th>Z15</th>\n",
       "      <th>Z16</th>\n",
       "      <th>Z17</th>\n",
       "      <th>Z18</th>\n",
       "      <th>Z19</th>\n",
       "      <th>Z20</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.624956</td>\n",
       "      <td>5.174488</td>\n",
       "      <td>7.273648</td>\n",
       "      <td>9.326124</td>\n",
       "      <td>3.932348</td>\n",
       "      <td>6.508001</td>\n",
       "      <td>8.425513</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.629178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.599319</td>\n",
       "      <td>0.506578</td>\n",
       "      <td>0.558672</td>\n",
       "      <td>0.602381</td>\n",
       "      <td>0.607569</td>\n",
       "      <td>0.534898</td>\n",
       "      <td>0.571628</td>\n",
       "      <td>0.598528</td>\n",
       "      <td>0.599285</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.583270</td>\n",
       "      <td>5.187694</td>\n",
       "      <td>7.324605</td>\n",
       "      <td>9.382927</td>\n",
       "      <td>4.005897</td>\n",
       "      <td>6.595340</td>\n",
       "      <td>8.504929</td>\n",
       "      <td>10.090124</td>\n",
       "      <td>2.664583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.603865</td>\n",
       "      <td>0.509563</td>\n",
       "      <td>0.563531</td>\n",
       "      <td>0.605887</td>\n",
       "      <td>0.610267</td>\n",
       "      <td>0.537730</td>\n",
       "      <td>0.575576</td>\n",
       "      <td>0.601663</td>\n",
       "      <td>0.601931</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.748158</td>\n",
       "      <td>5.363149</td>\n",
       "      <td>7.521592</td>\n",
       "      <td>9.593395</td>\n",
       "      <td>4.247943</td>\n",
       "      <td>6.859929</td>\n",
       "      <td>8.791570</td>\n",
       "      <td>10.364044</td>\n",
       "      <td>2.909677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608186</td>\n",
       "      <td>0.512301</td>\n",
       "      <td>0.568760</td>\n",
       "      <td>0.611362</td>\n",
       "      <td>0.614273</td>\n",
       "      <td>0.541641</td>\n",
       "      <td>0.580724</td>\n",
       "      <td>0.606767</td>\n",
       "      <td>0.606232</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.626798</td>\n",
       "      <td>5.300179</td>\n",
       "      <td>7.431179</td>\n",
       "      <td>9.456337</td>\n",
       "      <td>4.206511</td>\n",
       "      <td>6.819772</td>\n",
       "      <td>8.691635</td>\n",
       "      <td>10.217592</td>\n",
       "      <td>2.936363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614516</td>\n",
       "      <td>0.517558</td>\n",
       "      <td>0.573352</td>\n",
       "      <td>0.615064</td>\n",
       "      <td>0.621126</td>\n",
       "      <td>0.545935</td>\n",
       "      <td>0.584658</td>\n",
       "      <td>0.610591</td>\n",
       "      <td>0.612398</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.436093</td>\n",
       "      <td>5.077130</td>\n",
       "      <td>7.188783</td>\n",
       "      <td>9.232393</td>\n",
       "      <td>3.923741</td>\n",
       "      <td>6.527157</td>\n",
       "      <td>8.411973</td>\n",
       "      <td>9.963123</td>\n",
       "      <td>2.670181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.611038</td>\n",
       "      <td>0.514408</td>\n",
       "      <td>0.571130</td>\n",
       "      <td>0.610620</td>\n",
       "      <td>0.617288</td>\n",
       "      <td>0.541695</td>\n",
       "      <td>0.579587</td>\n",
       "      <td>0.606636</td>\n",
       "      <td>0.611063</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4678</th>\n",
       "      <td>2.593528</td>\n",
       "      <td>0.500697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.076023</td>\n",
       "      <td>2.081046</td>\n",
       "      <td>2.930423</td>\n",
       "      <td>3.378173</td>\n",
       "      <td>3.589213</td>\n",
       "      <td>3.777007</td>\n",
       "      <td>4.693532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438054</td>\n",
       "      <td>0.579991</td>\n",
       "      <td>0.525929</td>\n",
       "      <td>0.530355</td>\n",
       "      <td>0.546927</td>\n",
       "      <td>0.610211</td>\n",
       "      <td>0.579010</td>\n",
       "      <td>0.580965</td>\n",
       "      <td>0.594059</td>\n",
       "      <td>lihat1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4679</th>\n",
       "      <td>2.502447</td>\n",
       "      <td>0.228917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.071760</td>\n",
       "      <td>2.125836</td>\n",
       "      <td>2.087375</td>\n",
       "      <td>3.154392</td>\n",
       "      <td>3.785621</td>\n",
       "      <td>4.255047</td>\n",
       "      <td>3.942865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426719</td>\n",
       "      <td>0.573717</td>\n",
       "      <td>0.516804</td>\n",
       "      <td>0.526176</td>\n",
       "      <td>0.543807</td>\n",
       "      <td>0.603426</td>\n",
       "      <td>0.565350</td>\n",
       "      <td>0.571501</td>\n",
       "      <td>0.586730</td>\n",
       "      <td>lihat1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4680</th>\n",
       "      <td>2.748098</td>\n",
       "      <td>0.665617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.779981</td>\n",
       "      <td>1.824454</td>\n",
       "      <td>2.313113</td>\n",
       "      <td>3.477518</td>\n",
       "      <td>4.103491</td>\n",
       "      <td>4.544356</td>\n",
       "      <td>4.014791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426158</td>\n",
       "      <td>0.565734</td>\n",
       "      <td>0.516555</td>\n",
       "      <td>0.529514</td>\n",
       "      <td>0.549311</td>\n",
       "      <td>0.594870</td>\n",
       "      <td>0.569564</td>\n",
       "      <td>0.580029</td>\n",
       "      <td>0.597145</td>\n",
       "      <td>lihat1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4681</th>\n",
       "      <td>2.530922</td>\n",
       "      <td>0.620228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.920414</td>\n",
       "      <td>2.184480</td>\n",
       "      <td>2.050577</td>\n",
       "      <td>3.163178</td>\n",
       "      <td>3.808414</td>\n",
       "      <td>4.295339</td>\n",
       "      <td>3.669435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430082</td>\n",
       "      <td>0.568105</td>\n",
       "      <td>0.522663</td>\n",
       "      <td>0.535882</td>\n",
       "      <td>0.555317</td>\n",
       "      <td>0.597988</td>\n",
       "      <td>0.571828</td>\n",
       "      <td>0.582683</td>\n",
       "      <td>0.600482</td>\n",
       "      <td>lihat1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>2.503875</td>\n",
       "      <td>0.859203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546680</td>\n",
       "      <td>1.832024</td>\n",
       "      <td>1.706398</td>\n",
       "      <td>2.554486</td>\n",
       "      <td>3.032129</td>\n",
       "      <td>3.473086</td>\n",
       "      <td>3.222419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504724</td>\n",
       "      <td>0.697586</td>\n",
       "      <td>0.623289</td>\n",
       "      <td>0.638834</td>\n",
       "      <td>0.663707</td>\n",
       "      <td>0.724548</td>\n",
       "      <td>0.665965</td>\n",
       "      <td>0.672636</td>\n",
       "      <td>0.692294</td>\n",
       "      <td>lihat1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4683 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X0        X1        X2        X3        X4        X5        X6  \\\n",
       "0     0.000000  2.624956  5.174488  7.273648  9.326124  3.932348  6.508001   \n",
       "1     0.000000  2.583270  5.187694  7.324605  9.382927  4.005897  6.595340   \n",
       "2     0.000000  2.748158  5.363149  7.521592  9.593395  4.247943  6.859929   \n",
       "3     0.000000  2.626798  5.300179  7.431179  9.456337  4.206511  6.819772   \n",
       "4     0.000000  2.436093  5.077130  7.188783  9.232393  3.923741  6.527157   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4678  2.593528  0.500697  0.000000  1.076023  2.081046  2.930423  3.378173   \n",
       "4679  2.502447  0.228917  0.000000  1.071760  2.125836  2.087375  3.154392   \n",
       "4680  2.748098  0.665617  0.000000  0.779981  1.824454  2.313113  3.477518   \n",
       "4681  2.530922  0.620228  0.000000  0.920414  2.184480  2.050577  3.163178   \n",
       "4682  2.503875  0.859203  0.000000  0.546680  1.832024  1.706398  2.554486   \n",
       "\n",
       "            X7         X8        X9  ...       Z12       Z13       Z14  \\\n",
       "0     8.425513  10.000000  2.629178  ...  0.599319  0.506578  0.558672   \n",
       "1     8.504929  10.090124  2.664583  ...  0.603865  0.509563  0.563531   \n",
       "2     8.791570  10.364044  2.909677  ...  0.608186  0.512301  0.568760   \n",
       "3     8.691635  10.217592  2.936363  ...  0.614516  0.517558  0.573352   \n",
       "4     8.411973   9.963123  2.670181  ...  0.611038  0.514408  0.571130   \n",
       "...        ...        ...       ...  ...       ...       ...       ...   \n",
       "4678  3.589213   3.777007  4.693532  ...  0.438054  0.579991  0.525929   \n",
       "4679  3.785621   4.255047  3.942865  ...  0.426719  0.573717  0.516804   \n",
       "4680  4.103491   4.544356  4.014791  ...  0.426158  0.565734  0.516555   \n",
       "4681  3.808414   4.295339  3.669435  ...  0.430082  0.568105  0.522663   \n",
       "4682  3.032129   3.473086  3.222419  ...  0.504724  0.697586  0.623289   \n",
       "\n",
       "           Z15       Z16       Z17       Z18       Z19       Z20   Label  \n",
       "0     0.602381  0.607569  0.534898  0.571628  0.598528  0.599285       g  \n",
       "1     0.605887  0.610267  0.537730  0.575576  0.601663  0.601931       g  \n",
       "2     0.611362  0.614273  0.541641  0.580724  0.606767  0.606232       g  \n",
       "3     0.615064  0.621126  0.545935  0.584658  0.610591  0.612398       g  \n",
       "4     0.610620  0.617288  0.541695  0.579587  0.606636  0.611063       g  \n",
       "...        ...       ...       ...       ...       ...       ...     ...  \n",
       "4678  0.530355  0.546927  0.610211  0.579010  0.580965  0.594059  lihat1  \n",
       "4679  0.526176  0.543807  0.603426  0.565350  0.571501  0.586730  lihat1  \n",
       "4680  0.529514  0.549311  0.594870  0.569564  0.580029  0.597145  lihat1  \n",
       "4681  0.535882  0.555317  0.597988  0.571828  0.582683  0.600482  lihat1  \n",
       "4682  0.638834  0.663707  0.724548  0.665965  0.672636  0.692294  lihat1  \n",
       "\n",
       "[4683 rows x 64 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df.to_csv('csv/static_raw/filter_fix.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "481ae267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ABJAD\n",
    "column_numbersY =sorted([12,2,16,5,20,8,0,3,4,15,7,11,13,10,19,17])\n",
    "column_numbers =sorted([1,3,4,20,8,12,10,16,6,14,18,7,11])\n",
    "column_numbersZ=[2,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "7dfdb8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 6, 7, 8, 10, 11, 12, 14, 16, 18, 20]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "791f3931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 3, 4, 5, 7, 8, 10, 11, 12, 13, 15, 16, 17, 19, 20]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([12,2,16,5,20,8,0,3,4,15,7,11,13,10,19,17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "eabc106e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(column_numbers)+len(column_numbersY)+len(column_numbersZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26a1f12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 19, 20]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([12,2,16,5,6,20,8,0,3,4,15,7,11,13,10,19,17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87576256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_numbersY =sorted([12,16,19,20,8,15,14,18,2,13,5,6,9,17,1,3,0,4])\n",
    "column_numbers =sorted([4,20,19,18,3,14,13,15,5,17,11,7,8,12,])\n",
    "column_numbersZ=[4,5,8,12]\n",
    "len(column_numbers)+len(column_numbersY)+len(column_numbersZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95e041d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 16, 17, 17, 20]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([5,1,9,10,12,13,15,16,17,20,4,6,7,8,11,16,17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "f5f80c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>Z13</th>\n",
       "      <th>Z14</th>\n",
       "      <th>Z15</th>\n",
       "      <th>Z16</th>\n",
       "      <th>Z17</th>\n",
       "      <th>Z18</th>\n",
       "      <th>Z19</th>\n",
       "      <th>Z20</th>\n",
       "      <th>Sample Num</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.262496</td>\n",
       "      <td>0.517449</td>\n",
       "      <td>0.727365</td>\n",
       "      <td>0.932612</td>\n",
       "      <td>0.393235</td>\n",
       "      <td>0.650800</td>\n",
       "      <td>0.842551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.262918</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057151</td>\n",
       "      <td>-0.089256</td>\n",
       "      <td>-0.074899</td>\n",
       "      <td>-0.058334</td>\n",
       "      <td>-0.070132</td>\n",
       "      <td>-0.088469</td>\n",
       "      <td>-0.077601</td>\n",
       "      <td>-0.066305</td>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258327</td>\n",
       "      <td>0.518769</td>\n",
       "      <td>0.732461</td>\n",
       "      <td>0.938293</td>\n",
       "      <td>0.400590</td>\n",
       "      <td>0.659534</td>\n",
       "      <td>0.850493</td>\n",
       "      <td>1.009012</td>\n",
       "      <td>0.266458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057848</td>\n",
       "      <td>-0.089363</td>\n",
       "      <td>-0.074848</td>\n",
       "      <td>-0.058618</td>\n",
       "      <td>-0.070955</td>\n",
       "      <td>-0.088610</td>\n",
       "      <td>-0.077117</td>\n",
       "      <td>-0.065674</td>\n",
       "      <td>2</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274816</td>\n",
       "      <td>0.536315</td>\n",
       "      <td>0.752159</td>\n",
       "      <td>0.959339</td>\n",
       "      <td>0.424794</td>\n",
       "      <td>0.685993</td>\n",
       "      <td>0.879157</td>\n",
       "      <td>1.036404</td>\n",
       "      <td>0.290968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061454</td>\n",
       "      <td>-0.093907</td>\n",
       "      <td>-0.078565</td>\n",
       "      <td>-0.061478</td>\n",
       "      <td>-0.074942</td>\n",
       "      <td>-0.093178</td>\n",
       "      <td>-0.081417</td>\n",
       "      <td>-0.069630</td>\n",
       "      <td>3</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.262680</td>\n",
       "      <td>0.530018</td>\n",
       "      <td>0.743118</td>\n",
       "      <td>0.945634</td>\n",
       "      <td>0.420651</td>\n",
       "      <td>0.681977</td>\n",
       "      <td>0.869163</td>\n",
       "      <td>1.021759</td>\n",
       "      <td>0.293636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059761</td>\n",
       "      <td>-0.091205</td>\n",
       "      <td>-0.077570</td>\n",
       "      <td>-0.062268</td>\n",
       "      <td>-0.071356</td>\n",
       "      <td>-0.088959</td>\n",
       "      <td>-0.078023</td>\n",
       "      <td>-0.067256</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.243609</td>\n",
       "      <td>0.507713</td>\n",
       "      <td>0.718878</td>\n",
       "      <td>0.923239</td>\n",
       "      <td>0.392374</td>\n",
       "      <td>0.652716</td>\n",
       "      <td>0.841197</td>\n",
       "      <td>0.996312</td>\n",
       "      <td>0.267018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056987</td>\n",
       "      <td>-0.088836</td>\n",
       "      <td>-0.075183</td>\n",
       "      <td>-0.059511</td>\n",
       "      <td>-0.068158</td>\n",
       "      <td>-0.086596</td>\n",
       "      <td>-0.076351</td>\n",
       "      <td>-0.065696</td>\n",
       "      <td>5</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5287</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088521</td>\n",
       "      <td>0.238378</td>\n",
       "      <td>0.387401</td>\n",
       "      <td>0.445715</td>\n",
       "      <td>0.412194</td>\n",
       "      <td>0.492168</td>\n",
       "      <td>0.427603</td>\n",
       "      <td>0.363169</td>\n",
       "      <td>0.418116</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004367</td>\n",
       "      <td>-0.002546</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>-0.006921</td>\n",
       "      <td>-0.005112</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>0.003144</td>\n",
       "      <td>76</td>\n",
       "      <td>delete_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5288</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.077284</td>\n",
       "      <td>0.212639</td>\n",
       "      <td>0.373947</td>\n",
       "      <td>0.442660</td>\n",
       "      <td>0.369622</td>\n",
       "      <td>0.489487</td>\n",
       "      <td>0.419909</td>\n",
       "      <td>0.347020</td>\n",
       "      <td>0.387698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007689</td>\n",
       "      <td>-0.006332</td>\n",
       "      <td>-0.002150</td>\n",
       "      <td>-0.000515</td>\n",
       "      <td>-0.008608</td>\n",
       "      <td>-0.007784</td>\n",
       "      <td>-0.003251</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>77</td>\n",
       "      <td>delete_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5289</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042045</td>\n",
       "      <td>0.147402</td>\n",
       "      <td>0.297640</td>\n",
       "      <td>0.385955</td>\n",
       "      <td>0.285512</td>\n",
       "      <td>0.431621</td>\n",
       "      <td>0.365643</td>\n",
       "      <td>0.294203</td>\n",
       "      <td>0.325706</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010225</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.008171</td>\n",
       "      <td>-0.005575</td>\n",
       "      <td>-0.012218</td>\n",
       "      <td>-0.013398</td>\n",
       "      <td>-0.008820</td>\n",
       "      <td>-0.004770</td>\n",
       "      <td>78</td>\n",
       "      <td>delete_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5290</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027985</td>\n",
       "      <td>0.115774</td>\n",
       "      <td>0.250317</td>\n",
       "      <td>0.336940</td>\n",
       "      <td>0.216791</td>\n",
       "      <td>0.393710</td>\n",
       "      <td>0.333120</td>\n",
       "      <td>0.261263</td>\n",
       "      <td>0.266437</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014969</td>\n",
       "      <td>-0.017042</td>\n",
       "      <td>-0.012246</td>\n",
       "      <td>-0.008893</td>\n",
       "      <td>-0.016799</td>\n",
       "      <td>-0.017754</td>\n",
       "      <td>-0.012925</td>\n",
       "      <td>-0.008874</td>\n",
       "      <td>79</td>\n",
       "      <td>delete_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5291</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.070805</td>\n",
       "      <td>0.198071</td>\n",
       "      <td>0.289609</td>\n",
       "      <td>0.204714</td>\n",
       "      <td>0.363397</td>\n",
       "      <td>0.312960</td>\n",
       "      <td>0.242393</td>\n",
       "      <td>0.263717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013237</td>\n",
       "      <td>-0.013975</td>\n",
       "      <td>-0.009839</td>\n",
       "      <td>-0.007923</td>\n",
       "      <td>-0.014047</td>\n",
       "      <td>-0.015136</td>\n",
       "      <td>-0.011826</td>\n",
       "      <td>-0.009138</td>\n",
       "      <td>80</td>\n",
       "      <td>delete_all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5292 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X0        X1        X2        X3        X4        X5        X6  \\\n",
       "0     0.0  0.262496  0.517449  0.727365  0.932612  0.393235  0.650800   \n",
       "1     0.0  0.258327  0.518769  0.732461  0.938293  0.400590  0.659534   \n",
       "2     0.0  0.274816  0.536315  0.752159  0.959339  0.424794  0.685993   \n",
       "3     0.0  0.262680  0.530018  0.743118  0.945634  0.420651  0.681977   \n",
       "4     0.0  0.243609  0.507713  0.718878  0.923239  0.392374  0.652716   \n",
       "...   ...       ...       ...       ...       ...       ...       ...   \n",
       "5287  0.0  0.088521  0.238378  0.387401  0.445715  0.412194  0.492168   \n",
       "5288  0.0  0.077284  0.212639  0.373947  0.442660  0.369622  0.489487   \n",
       "5289  0.0  0.042045  0.147402  0.297640  0.385955  0.285512  0.431621   \n",
       "5290  0.0  0.027985  0.115774  0.250317  0.336940  0.216791  0.393710   \n",
       "5291  0.0  0.002837  0.070805  0.198071  0.289609  0.204714  0.363397   \n",
       "\n",
       "            X7        X8        X9  ...       Z13       Z14       Z15  \\\n",
       "0     0.842551  1.000000  0.262918  ... -0.057151 -0.089256 -0.074899   \n",
       "1     0.850493  1.009012  0.266458  ... -0.057848 -0.089363 -0.074848   \n",
       "2     0.879157  1.036404  0.290968  ... -0.061454 -0.093907 -0.078565   \n",
       "3     0.869163  1.021759  0.293636  ... -0.059761 -0.091205 -0.077570   \n",
       "4     0.841197  0.996312  0.267018  ... -0.056987 -0.088836 -0.075183   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5287  0.427603  0.363169  0.418116  ... -0.004367 -0.002546  0.000341   \n",
       "5288  0.419909  0.347020  0.387698  ... -0.007689 -0.006332 -0.002150   \n",
       "5289  0.365643  0.294203  0.325706  ... -0.010225 -0.011968 -0.008171   \n",
       "5290  0.333120  0.261263  0.266437  ... -0.014969 -0.017042 -0.012246   \n",
       "5291  0.312960  0.242393  0.263717  ... -0.013237 -0.013975 -0.009839   \n",
       "\n",
       "           Z16       Z17       Z18       Z19       Z20  Sample Num       Label  \n",
       "0    -0.058334 -0.070132 -0.088469 -0.077601 -0.066305           1           g  \n",
       "1    -0.058618 -0.070955 -0.088610 -0.077117 -0.065674           2           g  \n",
       "2    -0.061478 -0.074942 -0.093178 -0.081417 -0.069630           3           g  \n",
       "3    -0.062268 -0.071356 -0.088959 -0.078023 -0.067256           4           g  \n",
       "4    -0.059511 -0.068158 -0.086596 -0.076351 -0.065696           5           g  \n",
       "...        ...       ...       ...       ...       ...         ...         ...  \n",
       "5287  0.001100 -0.006921 -0.005112 -0.000347  0.003144          76  delete_all  \n",
       "5288 -0.000515 -0.008608 -0.007784 -0.003251  0.000129          77  delete_all  \n",
       "5289 -0.005575 -0.012218 -0.013398 -0.008820 -0.004770          78  delete_all  \n",
       "5290 -0.008893 -0.016799 -0.017754 -0.012925 -0.008874          79  delete_all  \n",
       "5291 -0.007923 -0.014047 -0.015136 -0.011826 -0.009138          80  delete_all  \n",
       "\n",
       "[5292 rows x 65 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('csv/static/filter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "1e226003-8777-4efc-be61-bac5bcd10667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X14</th>\n",
       "      <th>...</th>\n",
       "      <th>Y12</th>\n",
       "      <th>Y13</th>\n",
       "      <th>Y15</th>\n",
       "      <th>Y16</th>\n",
       "      <th>Y17</th>\n",
       "      <th>Y19</th>\n",
       "      <th>Y20</th>\n",
       "      <th>Z2</th>\n",
       "      <th>Z4</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.262496</td>\n",
       "      <td>0.727365</td>\n",
       "      <td>0.932612</td>\n",
       "      <td>0.650800</td>\n",
       "      <td>0.842551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604380</td>\n",
       "      <td>0.516195</td>\n",
       "      <td>0.403268</td>\n",
       "      <td>0.464002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555946</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>0.571197</td>\n",
       "      <td>0.597035</td>\n",
       "      <td>0.235107</td>\n",
       "      <td>0.552011</td>\n",
       "      <td>0.555778</td>\n",
       "      <td>-0.030791</td>\n",
       "      <td>-0.069095</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.258327</td>\n",
       "      <td>0.732461</td>\n",
       "      <td>0.938293</td>\n",
       "      <td>0.659534</td>\n",
       "      <td>0.850493</td>\n",
       "      <td>1.009012</td>\n",
       "      <td>0.599207</td>\n",
       "      <td>0.506286</td>\n",
       "      <td>0.392692</td>\n",
       "      <td>0.458638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563074</td>\n",
       "      <td>0.093413</td>\n",
       "      <td>0.573146</td>\n",
       "      <td>0.594959</td>\n",
       "      <td>0.233696</td>\n",
       "      <td>0.552107</td>\n",
       "      <td>0.553441</td>\n",
       "      <td>-0.030174</td>\n",
       "      <td>-0.068387</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.274816</td>\n",
       "      <td>0.752159</td>\n",
       "      <td>0.959339</td>\n",
       "      <td>0.685993</td>\n",
       "      <td>0.879157</td>\n",
       "      <td>1.036404</td>\n",
       "      <td>0.623592</td>\n",
       "      <td>0.522443</td>\n",
       "      <td>0.403985</td>\n",
       "      <td>0.484650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573364</td>\n",
       "      <td>0.095818</td>\n",
       "      <td>0.589182</td>\n",
       "      <td>0.603680</td>\n",
       "      <td>0.241943</td>\n",
       "      <td>0.566299</td>\n",
       "      <td>0.563631</td>\n",
       "      <td>-0.031808</td>\n",
       "      <td>-0.072603</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.262680</td>\n",
       "      <td>0.743118</td>\n",
       "      <td>0.945634</td>\n",
       "      <td>0.681977</td>\n",
       "      <td>0.869163</td>\n",
       "      <td>1.021759</td>\n",
       "      <td>0.608133</td>\n",
       "      <td>0.524926</td>\n",
       "      <td>0.413922</td>\n",
       "      <td>0.471710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570633</td>\n",
       "      <td>0.087743</td>\n",
       "      <td>0.573362</td>\n",
       "      <td>0.603555</td>\n",
       "      <td>0.229074</td>\n",
       "      <td>0.551087</td>\n",
       "      <td>0.560085</td>\n",
       "      <td>-0.031188</td>\n",
       "      <td>-0.068279</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.243609</td>\n",
       "      <td>0.718878</td>\n",
       "      <td>0.923239</td>\n",
       "      <td>0.652716</td>\n",
       "      <td>0.841197</td>\n",
       "      <td>0.996312</td>\n",
       "      <td>0.596476</td>\n",
       "      <td>0.502294</td>\n",
       "      <td>0.381764</td>\n",
       "      <td>0.458405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565304</td>\n",
       "      <td>0.084046</td>\n",
       "      <td>0.563218</td>\n",
       "      <td>0.596427</td>\n",
       "      <td>0.219946</td>\n",
       "      <td>0.543379</td>\n",
       "      <td>0.565428</td>\n",
       "      <td>-0.032659</td>\n",
       "      <td>-0.069065</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5287</th>\n",
       "      <td>0.088521</td>\n",
       "      <td>0.387401</td>\n",
       "      <td>0.445715</td>\n",
       "      <td>0.492168</td>\n",
       "      <td>0.427603</td>\n",
       "      <td>0.363169</td>\n",
       "      <td>0.494306</td>\n",
       "      <td>0.438426</td>\n",
       "      <td>0.369994</td>\n",
       "      <td>0.480192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324130</td>\n",
       "      <td>0.505321</td>\n",
       "      <td>0.452161</td>\n",
       "      <td>0.457788</td>\n",
       "      <td>0.643378</td>\n",
       "      <td>0.572444</td>\n",
       "      <td>0.579656</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>-0.003055</td>\n",
       "      <td>delete_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5288</th>\n",
       "      <td>0.077284</td>\n",
       "      <td>0.373947</td>\n",
       "      <td>0.442660</td>\n",
       "      <td>0.489487</td>\n",
       "      <td>0.419909</td>\n",
       "      <td>0.347020</td>\n",
       "      <td>0.505746</td>\n",
       "      <td>0.444245</td>\n",
       "      <td>0.374059</td>\n",
       "      <td>0.496255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357846</td>\n",
       "      <td>0.565648</td>\n",
       "      <td>0.485139</td>\n",
       "      <td>0.505348</td>\n",
       "      <td>0.705379</td>\n",
       "      <td>0.615878</td>\n",
       "      <td>0.640818</td>\n",
       "      <td>-0.004628</td>\n",
       "      <td>-0.009670</td>\n",
       "      <td>delete_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5289</th>\n",
       "      <td>0.042045</td>\n",
       "      <td>0.297640</td>\n",
       "      <td>0.385955</td>\n",
       "      <td>0.431621</td>\n",
       "      <td>0.365643</td>\n",
       "      <td>0.294203</td>\n",
       "      <td>0.462778</td>\n",
       "      <td>0.400892</td>\n",
       "      <td>0.329740</td>\n",
       "      <td>0.467832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334172</td>\n",
       "      <td>0.531756</td>\n",
       "      <td>0.459338</td>\n",
       "      <td>0.491177</td>\n",
       "      <td>0.685894</td>\n",
       "      <td>0.599529</td>\n",
       "      <td>0.636793</td>\n",
       "      <td>-0.004892</td>\n",
       "      <td>-0.010289</td>\n",
       "      <td>delete_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5290</th>\n",
       "      <td>0.027985</td>\n",
       "      <td>0.250317</td>\n",
       "      <td>0.336940</td>\n",
       "      <td>0.393710</td>\n",
       "      <td>0.333120</td>\n",
       "      <td>0.261263</td>\n",
       "      <td>0.425950</td>\n",
       "      <td>0.368199</td>\n",
       "      <td>0.298937</td>\n",
       "      <td>0.439138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354797</td>\n",
       "      <td>0.528763</td>\n",
       "      <td>0.461273</td>\n",
       "      <td>0.497419</td>\n",
       "      <td>0.670733</td>\n",
       "      <td>0.589383</td>\n",
       "      <td>0.631490</td>\n",
       "      <td>-0.006257</td>\n",
       "      <td>-0.013554</td>\n",
       "      <td>delete_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5291</th>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.198071</td>\n",
       "      <td>0.289609</td>\n",
       "      <td>0.363397</td>\n",
       "      <td>0.312960</td>\n",
       "      <td>0.242393</td>\n",
       "      <td>0.403042</td>\n",
       "      <td>0.353049</td>\n",
       "      <td>0.290108</td>\n",
       "      <td>0.421711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322066</td>\n",
       "      <td>0.509746</td>\n",
       "      <td>0.420875</td>\n",
       "      <td>0.466657</td>\n",
       "      <td>0.650570</td>\n",
       "      <td>0.552782</td>\n",
       "      <td>0.603697</td>\n",
       "      <td>-0.007912</td>\n",
       "      <td>-0.016154</td>\n",
       "      <td>delete_all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5292 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X1        X3        X4        X6        X7        X8       X10  \\\n",
       "0     0.262496  0.727365  0.932612  0.650800  0.842551  1.000000  0.604380   \n",
       "1     0.258327  0.732461  0.938293  0.659534  0.850493  1.009012  0.599207   \n",
       "2     0.274816  0.752159  0.959339  0.685993  0.879157  1.036404  0.623592   \n",
       "3     0.262680  0.743118  0.945634  0.681977  0.869163  1.021759  0.608133   \n",
       "4     0.243609  0.718878  0.923239  0.652716  0.841197  0.996312  0.596476   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5287  0.088521  0.387401  0.445715  0.492168  0.427603  0.363169  0.494306   \n",
       "5288  0.077284  0.373947  0.442660  0.489487  0.419909  0.347020  0.505746   \n",
       "5289  0.042045  0.297640  0.385955  0.431621  0.365643  0.294203  0.462778   \n",
       "5290  0.027985  0.250317  0.336940  0.393710  0.333120  0.261263  0.425950   \n",
       "5291  0.002837  0.198071  0.289609  0.363397  0.312960  0.242393  0.403042   \n",
       "\n",
       "           X11       X12       X14  ...       Y12       Y13       Y15  \\\n",
       "0     0.516195  0.403268  0.464002  ...  0.555946  0.094059  0.571197   \n",
       "1     0.506286  0.392692  0.458638  ...  0.563074  0.093413  0.573146   \n",
       "2     0.522443  0.403985  0.484650  ...  0.573364  0.095818  0.589182   \n",
       "3     0.524926  0.413922  0.471710  ...  0.570633  0.087743  0.573362   \n",
       "4     0.502294  0.381764  0.458405  ...  0.565304  0.084046  0.563218   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5287  0.438426  0.369994  0.480192  ...  0.324130  0.505321  0.452161   \n",
       "5288  0.444245  0.374059  0.496255  ...  0.357846  0.565648  0.485139   \n",
       "5289  0.400892  0.329740  0.467832  ...  0.334172  0.531756  0.459338   \n",
       "5290  0.368199  0.298937  0.439138  ...  0.354797  0.528763  0.461273   \n",
       "5291  0.353049  0.290108  0.421711  ...  0.322066  0.509746  0.420875   \n",
       "\n",
       "           Y16       Y17       Y19       Y20        Z2        Z4       Label  \n",
       "0     0.597035  0.235107  0.552011  0.555778 -0.030791 -0.069095           g  \n",
       "1     0.594959  0.233696  0.552107  0.553441 -0.030174 -0.068387           g  \n",
       "2     0.603680  0.241943  0.566299  0.563631 -0.031808 -0.072603           g  \n",
       "3     0.603555  0.229074  0.551087  0.560085 -0.031188 -0.068279           g  \n",
       "4     0.596427  0.219946  0.543379  0.565428 -0.032659 -0.069065           g  \n",
       "...        ...       ...       ...       ...       ...       ...         ...  \n",
       "5287  0.457788  0.643378  0.572444  0.579656  0.000313 -0.003055  delete_all  \n",
       "5288  0.505348  0.705379  0.615878  0.640818 -0.004628 -0.009670  delete_all  \n",
       "5289  0.491177  0.685894  0.599529  0.636793 -0.004892 -0.010289  delete_all  \n",
       "5290  0.497419  0.670733  0.589383  0.631490 -0.006257 -0.013554  delete_all  \n",
       "5291  0.466657  0.650570  0.552782  0.603697 -0.007912 -0.016154  delete_all  \n",
       "\n",
       "[5292 rows x 32 columns]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Membaca CSV\n",
    "df = pd.read_csv('csv/static/filter.csv')\n",
    "# column_numbers=sorted([4,20,19,18,3,14,2,13,15,5,17,6,7,8])\n",
    "\n",
    "# column_numbersY =sorted([12,2,1,16,5,6,20,8,0,3,4,15,9,7,11,13,10,19,14,17])\n",
    "# # Menentukan nomor urut kolom yang ingin dipilih\n",
    "# # column_numbers = sorted([5,1,9,10,12,13,15,16,17,20,4,6,7,8,11,16,17])  # Ganti dengan indeks kolom yang diinginkan\n",
    "# # column_numbersY = sorted([2,3,4,7,9,10,11,12,15,19,20,16,17])\n",
    "# # column_numbersZ= [4,7,11]\n",
    "# column_numbersZ=[5,8,12,20]\n",
    "# Menyaring kolom berdasarkan nomor urut dan menambahkan 'X' sebagai prefix\n",
    "columnsX= [f'X{num}' for num in column_numbers]\n",
    "columnsY=[f'Y{num}' for num in column_numbersY]\n",
    "columnsZ=[f'Z{num}' for num in column_numbersZ]\n",
    "# Memilih kolom yang sesuai dengan nama kolom yang sudah dihasilkan\n",
    "df= df[columnsX+columnsY+columnsZ+['Label']]\n",
    "\n",
    "# Menampilkan hasil\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b00cc35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'space', 't', 'tidak1', 'to_symbol', 'u', 'v', 'w', 'x', 'y', 'z']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df.Label.unique())[30:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8357c8",
   "metadata": {},
   "source": [
    "## TRANSFORM DATAFRAME PANDAS CSV TO NUMPY ARRAY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "27c06f16-e2f4-4eb5-9231-aa6430472165",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, di in enumerate(df['Label'].unique()):\n",
    "    for a in range(df[df.Label == di].shape[0]):\n",
    "        # print(np.array(df[df.Label == 'a'].iloc[a].to_list()))\n",
    "        data.append(df[df.Label == di].iloc[a,:-1].to_list())\n",
    " \n",
    "    # Iterasi untuk setiap nilai unik di 'Sub 2'\n",
    "      \n",
    "\n",
    "all_Data = np.array(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11018685",
   "metadata": {},
   "source": [
    "## ENCODED AND TO CATEGORI LABEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "8d9c8e82-2e04-460c-a0e8-75b79fa76250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 11 11 ...  8  8  8]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = np.array(df.Label.to_list())\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "print(y_encoded)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_encoded = to_categorical(y_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff32c23",
   "metadata": {},
   "source": [
    "## CREATE LABEL MAP UNTUK IMPLEMENTASI NANTI DAN SAVE TO PKL AND LOAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "601029dc-2104-494a-b7c5-55e7786916bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '10_2',\n",
       " 1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'backspace',\n",
       " 4: 'c',\n",
       " 5: 'cepat1',\n",
       " 6: 'cepat2',\n",
       " 7: 'd',\n",
       " 8: 'delete_all',\n",
       " 9: 'e',\n",
       " 10: 'f',\n",
       " 11: 'g',\n",
       " 12: 'h',\n",
       " 13: 'i',\n",
       " 14: 'j2',\n",
       " 15: 'k',\n",
       " 16: 'l',\n",
       " 17: 'lihat1',\n",
       " 18: 'lihat2',\n",
       " 19: 'm',\n",
       " 20: 'menang1',\n",
       " 21: 'menang2',\n",
       " 22: 'n',\n",
       " 23: 'o',\n",
       " 24: 'p',\n",
       " 25: 'paham1',\n",
       " 26: 'paham2',\n",
       " 27: 'percaya',\n",
       " 28: 'q',\n",
       " 29: 'r',\n",
       " 30: 's',\n",
       " 31: 'space',\n",
       " 32: 't',\n",
       " 33: 'tidak1',\n",
       " 34: 'to_symbol',\n",
       " 35: 'u',\n",
       " 36: 'v',\n",
       " 37: 'w',\n",
       " 38: 'x',\n",
       " 39: 'y',\n",
       " 40: 'z'}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_LABELMAP = 'csv/label map/static.pkl'\n",
    "label_map = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "label_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "60463401-764b-4d07-babe-a93f1c64c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "label_map = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "\n",
    "# Simpan ke file\n",
    "with open(PATH_LABELMAP ,'wb') as f:\n",
    "    pickle.dump(label_map, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bd0f336f-63ef-4220-8e10-c10ca7a87854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dari file\n",
    "with open(PATH_LABELMAP , 'rb') as f:\n",
    "    label_map = pickle.load(f)\n",
    "\n",
    "\n",
    "len(label_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e6a43",
   "metadata": {},
   "source": [
    "## SPLIT DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "a31060de-c853-40db-a640-e036fa4cbb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = np.array(df.Label.to_list())\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_Data, y_encoded, test_size=0.2, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "37e9d6f5-620d-4356-810b-3beb88a74737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 31)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "np.unique(y_test, return_counts=True)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716fa545",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARY TO CREATE MODEL ARCHITECTURE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "9f200613-3e8a-411d-b0d4-26183c1037b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c591f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ea1009",
   "metadata": {},
   "source": [
    "## CREATE MODEL ARCHITECTURE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "16c77c3b-7fa0-4511-8cbe-1e40ebe7bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(31,), activation='relu')) \n",
    "\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dense(len(label_map), activation='softmax'))  # 4 kelas output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "e60eadaa-42b6-4031-933e-a827c00cda14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_32\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_32\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_108 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_109 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_110 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_111 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,289</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_108 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_109 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_32 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_110 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_111 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m)             │         \u001b[38;5;34m5,289\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,817</span> (77.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,817\u001b[0m (77.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,817</span> (77.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,817\u001b[0m (77.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(31,), activation='relu', \n",
    "                kernel_regularizer=l2(0.01)))  # L2 regularization\n",
    "\n",
    "model.add(Dense(64, activation='relu'))  # L2 regularization\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01)))  # L2 regularization\n",
    "\n",
    "\n",
    "model.add(Dense(len(label_map), activation='softmax'))  # Output layer, tanpa regularisasi karena biasanya hanya digunakan pada hidden layers\n",
    "\n",
    "# Ringkasan model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "1990d087-b054-409e-bf65-e7120c77437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "e3690b75-7738-41a6-bdbc-42398d8e9a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.0906 - loss: 3.4914 - val_accuracy: 0.4245 - val_loss: 2.3695\n",
      "Epoch 2/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4989 - loss: 2.0625 - val_accuracy: 0.8278 - val_loss: 1.0463\n",
      "Epoch 3/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7555 - loss: 1.0068 - val_accuracy: 0.9387 - val_loss: 0.5272\n",
      "Epoch 4/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8702 - loss: 0.6053 - val_accuracy: 0.9741 - val_loss: 0.3223\n",
      "Epoch 5/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9117 - loss: 0.4094 - val_accuracy: 0.9693 - val_loss: 0.2141\n",
      "Epoch 6/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9263 - loss: 0.3219 - val_accuracy: 0.9835 - val_loss: 0.1604\n",
      "Epoch 7/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9190 - loss: 0.2842 - val_accuracy: 0.9788 - val_loss: 0.1328\n",
      "Epoch 8/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9423 - loss: 0.2273 - val_accuracy: 0.9858 - val_loss: 0.1030\n",
      "Epoch 9/50\n",
      "\u001b[1m106/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9645 - loss: 0.1802\n",
      "✅ Val_accuracy mencapai 0.99 — Stop training.\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9641 - loss: 0.1796 - val_accuracy: 0.9929 - val_loss: 0.0806\n",
      "Restoring model weights from the end of the best epoch: 9.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=0.001,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Custom Callback: Stop if accuracy or loss sudah cukup bagus\n",
    "class SmartStop(Callback):\n",
    "    def __init__(self, target_acc=0.99, target_loss=0.04):\n",
    "        super().__init__()\n",
    "        self.target_acc = target_acc\n",
    "        self.target_loss = target_loss\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_acc = logs.get(\"val_accuracy\")\n",
    "        val_loss = logs.get(\"val_loss\")\n",
    "\n",
    "        if val_acc is not None and val_acc >= self.target_acc:\n",
    "            print(f\"\\n✅ Val_accuracy mencapai {val_acc:.2f} — Stop training.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "        elif val_loss is not None and val_loss <= self.target_loss:\n",
    "            print(f\"\\n✅ Val_loss cukup kecil ({val_loss:.4f}) — Stop training.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Gabungkan keduanya\n",
    "callbacks = [early_stop, SmartStop(target_acc=0.99, target_loss=0.04)]\n",
    "\n",
    "# Fit model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "ae7fe037-5b47-48cb-a5f2-2fd3cef0d208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Grafik Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Grafik Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "149a1ad1-7736-47c3-b6d2-2278e46ffcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9950 - loss: 0.0792\n",
      "Akurasi Test: 0.99\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Akurasi Test: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "c6ed8cda-335f-45d4-a4af-2005f1b7ad8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"model/static/model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "847abde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20898108",
   "metadata": {},
   "source": [
    "## CUSTOM MODE WITHOUT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181a086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.1766, Accuracy: 35.00%\n",
      "Epoch 2/20, Loss: 1.1743, Accuracy: 34.60%\n",
      "Epoch 3/20, Loss: 1.1711, Accuracy: 34.30%\n",
      "Epoch 4/20, Loss: 1.1694, Accuracy: 35.60%\n",
      "Epoch 5/20, Loss: 1.1723, Accuracy: 36.30%\n",
      "Epoch 6/20, Loss: 1.1670, Accuracy: 37.00%\n",
      "Epoch 7/20, Loss: 1.1660, Accuracy: 36.70%\n",
      "Epoch 8/20, Loss: 1.1659, Accuracy: 35.70%\n",
      "Epoch 9/20, Loss: 1.1698, Accuracy: 37.50%\n",
      "Epoch 10/20, Loss: 1.1644, Accuracy: 37.10%\n",
      "Epoch 11/20, Loss: 1.1608, Accuracy: 38.40%\n",
      "Epoch 12/20, Loss: 1.1661, Accuracy: 39.40%\n",
      "Epoch 13/20, Loss: 1.1667, Accuracy: 37.30%\n",
      "Epoch 14/20, Loss: 1.1647, Accuracy: 36.60%\n",
      "Epoch 15/20, Loss: 1.1654, Accuracy: 40.70%\n",
      "Epoch 16/20, Loss: 1.1646, Accuracy: 40.10%\n",
      "Epoch 17/20, Loss: 1.1639, Accuracy: 39.00%\n",
      "Epoch 18/20, Loss: 1.1611, Accuracy: 38.20%\n",
      "Epoch 19/20, Loss: 1.1643, Accuracy: 37.70%\n",
      "Epoch 20/20, Loss: 1.1634, Accuracy: 38.40%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# === Tensor ===\n",
    "class Tensor:\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    def __repr__(self):\n",
    "        return str(self.data)\n",
    "\n",
    "# === Base Layer ===\n",
    "class Layer:\n",
    "    def forward(self, input): raise NotImplementedError\n",
    "    def backward(self, grad_output, learning_rate): raise NotImplementedError\n",
    "\n",
    "# === Dense Layer ===\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.1\n",
    "        self.biases = np.zeros(output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(input, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(self.input.T, grad_output)\n",
    "        grad_biases = grad_output.sum(axis=0)\n",
    "\n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.biases -= learning_rate * grad_biases\n",
    "        return grad_input\n",
    "\n",
    "# === ReLU Activation ===\n",
    "class ReLU(Layer):\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.maximum(0, input)\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        return grad_output * (self.input > 0)\n",
    "\n",
    "# === Dropout ===\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "    def forward(self, input):\n",
    "        self.mask = (np.random.rand(*input.shape) > self.rate).astype(float)\n",
    "        return input * self.mask\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        return grad_output * self.mask\n",
    "\n",
    "# === Softmax ===\n",
    "class Softmax(Layer):\n",
    "    def forward(self, input):\n",
    "        exps = np.exp(input - np.max(input, axis=1, keepdims=True))\n",
    "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        return grad_output  # digunakan bersamaan dengan cross-entropy\n",
    "\n",
    "# === Batch Norm (optional layer) ===\n",
    "class BatchNorm(Layer):\n",
    "    def __init__(self, input_size):\n",
    "        self.gamma = np.ones(input_size)\n",
    "        self.beta = np.zeros(input_size)\n",
    "        self.eps = 1e-5\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.mean = input.mean(axis=0)\n",
    "        self.var = input.var(axis=0)\n",
    "        self.norm = (input - self.mean) / np.sqrt(self.var + self.eps)\n",
    "        return self.gamma * self.norm + self.beta\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        # Simpler version for illustration\n",
    "        grad_input = grad_output * self.gamma\n",
    "        return grad_input\n",
    "\n",
    "# === Loss: Cross Entropy ===\n",
    "class CrossEntropy:\n",
    "    def forward(self, prediction, target):\n",
    "        self.pred = prediction\n",
    "        self.target = target\n",
    "        log_likelihood = -np.log(prediction[range(len(target)), target] + 1e-9)\n",
    "        return np.mean(log_likelihood)\n",
    "\n",
    "    def backward(self):\n",
    "        grad = self.pred.copy()\n",
    "        grad[range(len(self.target)), self.target] -= 1\n",
    "        return grad / len(self.target)\n",
    "\n",
    "# === Optimizer: SGD ===\n",
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.lr = learning_rate\n",
    "\n",
    "# === Model ===\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.compiled = False\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def compile(self, optimizer, loss):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss\n",
    "        self.compiled = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_grad = layer.backward(loss_grad, self.optimizer.lr)\n",
    "\n",
    "    def fit(self, X, y, epochs=10, batch_size=32):\n",
    "        for epoch in range(epochs):\n",
    "            loss_total = 0\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                x_batch = X[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "\n",
    "                output = self.forward(x_batch)\n",
    "                loss = self.loss_fn.forward(output, y_batch)\n",
    "                grad = self.loss_fn.backward()\n",
    "                self.backward(grad)\n",
    "                loss_total += loss\n",
    "\n",
    "            avg_loss = loss_total / (len(X) // batch_size)\n",
    "            acc = self.evaluate(X, y)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {acc:.2%}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        return np.mean(pred == y)\n",
    "\n",
    "# === Simulasi Dataset (Dummy, 3 kelas, 10 fitur) ===\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(1000, 10)\n",
    "y = np.random.randint(0, 3, size=1000)\n",
    "\n",
    "# === Arsitektur ===\n",
    "model = Model()\n",
    "model.add(Dense(10, 64))\n",
    "model.add(BatchNorm(64))\n",
    "model.add(ReLU())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, 32))\n",
    "model.add(ReLU())\n",
    "model.add(Dense(32, 3))\n",
    "model.add(Softmax())\n",
    "\n",
    "# === Training ===\n",
    "model.compile(optimizer=SGD(learning_rate=0.05), loss=CrossEntropy())\n",
    "model.fit(X, y, epochs=20, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50c53979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Loss: 1.1345 - Accuracy: 34.40%\n",
      "Epoch 2/25 - Loss: 1.1342 - Accuracy: 35.50%\n",
      "Epoch 3/25 - Loss: 1.1335 - Accuracy: 36.20%\n",
      "Epoch 4/25 - Loss: 1.1322 - Accuracy: 37.60%\n",
      "Epoch 5/25 - Loss: 1.1320 - Accuracy: 37.40%\n",
      "Epoch 6/25 - Loss: 1.1315 - Accuracy: 36.50%\n",
      "Epoch 7/25 - Loss: 1.1309 - Accuracy: 37.30%\n",
      "Epoch 8/25 - Loss: 1.1298 - Accuracy: 38.10%\n",
      "Epoch 9/25 - Loss: 1.1297 - Accuracy: 37.40%\n",
      "Epoch 10/25 - Loss: 1.1278 - Accuracy: 37.60%\n",
      "Epoch 11/25 - Loss: 1.1281 - Accuracy: 40.10%\n",
      "Epoch 12/25 - Loss: 1.1254 - Accuracy: 41.20%\n",
      "Epoch 13/25 - Loss: 1.1259 - Accuracy: 39.60%\n",
      "Epoch 14/25 - Loss: 1.1249 - Accuracy: 40.10%\n",
      "Epoch 15/25 - Loss: 1.1232 - Accuracy: 39.10%\n",
      "Epoch 16/25 - Loss: 1.1226 - Accuracy: 40.00%\n",
      "Epoch 17/25 - Loss: 1.1218 - Accuracy: 40.30%\n",
      "Epoch 18/25 - Loss: 1.1183 - Accuracy: 39.90%\n",
      "Epoch 19/25 - Loss: 1.1192 - Accuracy: 40.90%\n",
      "Epoch 20/25 - Loss: 1.1184 - Accuracy: 41.00%\n",
      "Epoch 21/25 - Loss: 1.1173 - Accuracy: 41.00%\n",
      "Epoch 22/25 - Loss: 1.1179 - Accuracy: 41.10%\n",
      "Epoch 23/25 - Loss: 1.1134 - Accuracy: 42.20%\n",
      "Epoch 24/25 - Loss: 1.1146 - Accuracy: 41.90%\n",
      "Epoch 25/25 - Loss: 1.1124 - Accuracy: 41.50%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ======================= TENSOR & UTILITIES =======================\n",
    "class Tensor:\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    def __repr__(self):\n",
    "        return str(self.data)\n",
    "\n",
    "# ======================= BASE LAYER =======================\n",
    "class Layer:\n",
    "    def forward(self, input): raise NotImplementedError\n",
    "    def backward(self, grad_output, learning_rate): raise NotImplementedError\n",
    "\n",
    "# ======================= DENSE LAYER =======================\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.1\n",
    "        self.biases = np.zeros(output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(input, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(self.input.T, grad_output)\n",
    "        grad_biases = grad_output.sum(axis=0)\n",
    "\n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.biases -= learning_rate * grad_biases\n",
    "        return grad_input\n",
    "\n",
    "# ======================= ACTIVATION FUNCTIONS =======================\n",
    "class ReLU(Layer):\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        return grad_output * (self.input > 0)\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def forward(self, input):\n",
    "        exps = np.exp(input - np.max(input, axis=1, keepdims=True))\n",
    "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        return grad_output  # Gradient will be handled in loss\n",
    "\n",
    "# ======================= DROPOUT =======================\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.mask = (np.random.rand(*input.shape) > self.rate).astype(float)\n",
    "        return input * self.mask\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        return grad_output * self.mask\n",
    "\n",
    "# ======================= LOSS =======================\n",
    "class CrossEntropy:\n",
    "    def forward(self, prediction, target):\n",
    "        self.pred = prediction\n",
    "        self.target = target\n",
    "        log_likelihood = -np.log(prediction[range(len(target)), target] + 1e-9)\n",
    "        return np.mean(log_likelihood)\n",
    "\n",
    "    def backward(self):\n",
    "        grad = self.pred.copy()\n",
    "        grad[range(len(self.target)), self.target] -= 1\n",
    "        return grad / len(self.target)\n",
    "\n",
    "# ======================= OPTIMIZER =======================\n",
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.lr = learning_rate\n",
    "\n",
    "# ======================= MODEL =======================\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.compiled = False\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def compile(self, optimizer, loss):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss\n",
    "        self.compiled = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad, self.optimizer.lr)\n",
    "\n",
    "    def fit(self, X, y, epochs=20, batch_size=32):\n",
    "        for epoch in range(epochs):\n",
    "            loss_total = 0\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                x_batch = X[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "\n",
    "                out = self.forward(x_batch)\n",
    "                loss = self.loss_fn.forward(out, y_batch)\n",
    "                grad = self.loss_fn.backward()\n",
    "                self.backward(grad)\n",
    "\n",
    "                loss_total += loss\n",
    "\n",
    "            avg_loss = loss_total / (len(X) // batch_size)\n",
    "            acc = self.evaluate(X, y)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Accuracy: {acc:.2%}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        return np.mean(pred == y)\n",
    "\n",
    "# ======================= DATA (10 FEATURES, 3 KELAS) =======================\n",
    "np.random.seed(1)\n",
    "X = np.random.randn(1000, 10)\n",
    "y = np.random.randint(0, 3, size=1000)\n",
    "\n",
    "# ======================= MODEL STRUCTURE =======================\n",
    "model = Model()\n",
    "model.add(Dense(10, 64))\n",
    "model.add(ReLU())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, 64))\n",
    "model.add(ReLU())\n",
    "model.add(Dense(64, 32))\n",
    "model.add(ReLU())\n",
    "model.add(Dense(32, 3))\n",
    "model.add(Softmax())\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.05), loss=CrossEntropy())\n",
    "model.fit(X, y, epochs=25, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223c4f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Tensor(\"custom_model_1_1/dense_1_1/Add:0\", shape=(None, 1), dtype=float32)\n",
      "Tensor(\"custom_model_1_1/dense_1_1/Add:0\", shape=(None, 1), dtype=float32)\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - mae: 0.6022 - loss: 0.5257  \n",
      "Epoch 2/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - mae: 0.3908 - loss: 0.2382 \n",
      "Epoch 3/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - mae: 0.3887 - loss: 0.2276 \n",
      "Epoch 4/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - mae: 0.3680 - loss: 0.2079 \n",
      "Epoch 5/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - mae: 0.3681 - loss: 0.2069 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x78de7c3374a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "     \n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compute_loss(y=y, y_pred=y_pred)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        print(y_pred)\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        for metric in self.metrics:\n",
    "            if metric.name == \"loss\":\n",
    "                metric.update_state(loss)\n",
    "            else:\n",
    "                metric.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "# Construct and compile an instance of CustomModel\n",
    "inputs = keras.Input(shape=(32,))\n",
    "outputs = keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# Just use `fit` as usual\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 1))\n",
    "model.fit(x, y, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e87dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7196229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.3979 - mae: 1.0844 \n",
      "Epoch 2/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6027 - mae: 0.6549 \n",
      "Epoch 3/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2892 - mae: 0.4344 \n",
      "Epoch 4/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2081 - mae: 0.3712 \n",
      "Epoch 5/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2093 - mae: 0.3645 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7ec5c8356930>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute our own loss\n",
    "            mse_fn = keras.losses.MeanSquaredError()\n",
    "            loss = mse_fn(y, y_pred)\n",
    "\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.mae_metric.update_state(y, y_pred)\n",
    "        return {\"loss\": self.loss_tracker.result(), \"mae\": self.mae_metric.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [self.loss_tracker, self.mae_metric]\n",
    "\n",
    "\n",
    "# Construct an instance of CustomModel\n",
    "inputs = keras.Input(shape=(32,))\n",
    "outputs = keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(inputs, outputs)\n",
    "\n",
    "# We don't pass a loss or metrics here.\n",
    "model.compile(optimizer=\"adam\")\n",
    "\n",
    "# Just use `fit` as usual -- you can use callbacks, etc.\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 1))\n",
    "model.fit(x, y, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f55ee",
   "metadata": {},
   "source": [
    "## FILE AND FOLDER EXPLAINED\n",
    "\n",
    "1. **Data Raw**  \n",
    "   Lokasi: `csv/static/raw.csv`  \n",
    "   File ini berisi data landmark mentah yang belum diproses. Setiap baris dalam file ini berisi koordinat landmark yang diperoleh dari input video.\n",
    "\n",
    "2. **Data Filter**  \n",
    "   Lokasi: `csv/static/filter.csv`  \n",
    "   File ini berisi data landmark yang telah difilter untuk menghapus noise dan data yang tidak relevan. Di dalam folder ini, terdapat sublist file yang relevan:\n",
    "   \n",
    "   - **`filter_v1.csv`**  \n",
    "     File ini berisi hasil filter versi 1 yang diterapkan untuk data landmark. Ini mencakup **penghapusan nilai ekstrem dan koreksi posisi yang tidak valid.**\n",
    "   \n",
    "   - **`filter_v2.csv`**  \n",
    "     Versi kedua dari file filter ini mencakup **penghapusan nilai ekstrem dan koreksi posisi yang tidak valid.**\n",
    "3. **Data Label Terpisah (Separated)**  \n",
    "   Lokasi: `csv_separated/static/{target}/pure.csv`  \n",
    "   File ini berisi data landmark mentah yang belum diproses. Setiap baris dalam file ini berisi koordinat landmark yang diperoleh dari input video.\n",
    "   \n",
    "[Daftar Isi](#Daftar-Isi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
